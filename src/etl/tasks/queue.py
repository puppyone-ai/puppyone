"""
ETL Task Queue

Manages asynchronous ETL task queue and workers.
"""

import asyncio
import logging
from typing import Callable, Optional

from src.etl.tasks.models import ETLTask, ETLTaskStatus
from src.etl.tasks.repository import ETLTaskRepositoryBase

logger = logging.getLogger(__name__)


class ETLQueue:
    """Asynchronous task queue for ETL processing."""

    def __init__(
        self,
        task_repository: ETLTaskRepositoryBase,
        max_size: int = 1000,
        worker_count: int = 3,
    ):
        """
        Initialize ETL queue.

        Args:
            task_repository: Repository for task persistence
            max_size: Maximum queue size
            worker_count: Number of worker tasks
        """
        self.task_repository = task_repository
        self.queue: asyncio.Queue[int] = asyncio.Queue(maxsize=max_size)
        self.tasks: dict[int, ETLTask] = {}
        self.worker_count = worker_count
        self.workers: list[asyncio.Task] = []
        self.executor: Optional[Callable[[ETLTask], None]] = None
        self.is_running = False

        logger.info(
            f"ETLQueue initialized with max_size={max_size}, "
            f"worker_count={worker_count}"
        )

    def set_executor(self, executor: Callable[[ETLTask], None]):
        """
        Set the executor function for processing tasks.

        Args:
            executor: Async function that takes ETLTask and processes it
        """
        self.executor = executor
        logger.info("ETL queue executor set")

    async def submit(self, task: ETLTask) -> ETLTask:
        """
        Submit a task to the queue.

        Args:
            task: ETL task to submit (task_id will be assigned if None)

        Returns:
            Task with assigned task_id

        Raises:
            asyncio.QueueFull: If queue is full
        """
        # Create task in database first to get task_id
        task_with_id = self.task_repository.create_task(task)
        
        # Add to memory cache
        self.tasks[task_with_id.task_id] = task_with_id
        
        # Add to queue
        await self.queue.put(task_with_id.task_id)
        
        logger.info(f"Task {task_with_id.task_id} submitted to queue")
        return task_with_id

    def get_task(self, task_id: int) -> Optional[ETLTask]:
        """
        Get task by ID from memory cache.

        Args:
            task_id: Task identifier

        Returns:
            ETLTask if found, None otherwise
        """
        # First try memory cache
        task = self.tasks.get(task_id)
        
        # If not in memory, try database
        if task is None:
            task = self.task_repository.get_task(task_id)
            if task:
                # Cache it in memory
                self.tasks[task_id] = task
        
        return task

    def list_tasks(
        self,
        user_id: Optional[int] = None,
        project_id: Optional[int] = None,
        status: Optional[ETLTaskStatus] = None,
    ) -> list[ETLTask]:
        """
        List tasks with optional filters.

        Args:
            user_id: Filter by user ID
            project_id: Filter by project ID
            status: Filter by status

        Returns:
            List of matching tasks
        """
        # Use memory cache for fast access
        tasks = list(self.tasks.values())

        if user_id is not None:
            tasks = [t for t in tasks if t.user_id == user_id]
        if project_id is not None:
            tasks = [t for t in tasks if t.project_id == project_id]
        if status is not None:
            tasks = [t for t in tasks if t.status == status]

        return tasks

    async def start_workers(self):
        """Start worker tasks."""
        if self.is_running:
            logger.warning("Workers already running")
            return

        if not self.executor:
            raise RuntimeError("Executor not set. Call set_executor() first.")

        self.is_running = True
        logger.info(f"Starting {self.worker_count} workers")

        for i in range(self.worker_count):
            worker = asyncio.create_task(self._worker(i))
            self.workers.append(worker)

        logger.info(f"{self.worker_count} workers started")

    async def stop_workers(self):
        """Stop all worker tasks gracefully."""
        if not self.is_running:
            logger.warning("Workers not running")
            return

        logger.info("Stopping workers...")
        self.is_running = False

        # Cancel all workers
        for worker in self.workers:
            worker.cancel()

        # Wait for workers to finish
        await asyncio.gather(*self.workers, return_exceptions=True)

        self.workers.clear()
        logger.info("All workers stopped")

    async def _worker(self, worker_id: int):
        """
        Worker task that processes items from the queue.

        Args:
            worker_id: Worker identifier for logging
        """
        logger.info(f"Worker {worker_id} started")

        while self.is_running:
            try:
                # Get task from queue with timeout
                try:
                    task_id = await asyncio.wait_for(
                        self.queue.get(),
                        timeout=1.0
                    )
                except asyncio.TimeoutError:
                    continue

                task = self.tasks.get(task_id)
                if not task:
                    logger.error(f"Worker {worker_id}: Task {task_id} not found")
                    self.queue.task_done()
                    continue

                logger.info(
                    f"Worker {worker_id} processing task {task_id} "
                    f"(user_id={task.user_id}, filename={task.filename})"
                )

                # Execute task
                try:
                    if self.executor:
                        await self.executor(task)
                        
                        # Update database if task completed or failed
                        # (intermediate states are only in memory)
                        if task.status in (ETLTaskStatus.COMPLETED, ETLTaskStatus.FAILED):
                            try:
                                self.task_repository.update_task(task)
                                logger.info(
                                    f"Task {task_id} status persisted: {task.status.value}"
                                )
                            except Exception as e:
                                logger.error(
                                    f"Failed to persist task {task_id} status: {e}"
                                )
                except Exception as e:
                    logger.error(
                        f"Worker {worker_id}: Error processing task {task_id}: {e}",
                        exc_info=True
                    )
                    task.mark_failed(f"Worker error: {str(e)}")
                    
                    # Persist failure to database
                    try:
                        self.task_repository.update_task(task)
                    except Exception as db_error:
                        logger.error(
                            f"Failed to persist task {task_id} failure: {db_error}"
                        )

                self.queue.task_done()

            except asyncio.CancelledError:
                logger.info(f"Worker {worker_id} cancelled")
                break
            except Exception as e:
                logger.error(
                    f"Worker {worker_id}: Unexpected error: {e}",
                    exc_info=True
                )
                await asyncio.sleep(1)

        logger.info(f"Worker {worker_id} stopped")

    async def wait_empty(self):
        """Wait for queue to be empty."""
        await self.queue.join()

    def queue_size(self) -> int:
        """Get current queue size."""
        return self.queue.qsize()

    def task_count(self) -> int:
        """Get total number of tasks."""
        return len(self.tasks)

