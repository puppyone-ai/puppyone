"""ETLè§„åˆ™å¼•æ“å•å…ƒæµ‹è¯•

æµ‹è¯•è§„åˆ™å¼•æ“çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
- Promptæ„é€ 
- JSON SchemaéªŒè¯
- LLMè°ƒç”¨å’Œé‡è¯•æœºåˆ¶
- é”™è¯¯å¤„ç†
"""

import json
from datetime import datetime, UTC
from unittest.mock import AsyncMock, Mock, patch

import pytest
from jsonschema import ValidationError

from src.upload.file.rules.engine import RuleEngine
from src.upload.file.rules.schemas import ETLRule, TransformationResult
from src.llm.exceptions import LLMError
from src.llm.schemas import TextModelResponse


# ============= Fixtures =============


@pytest.fixture
def mock_llm_service():
    """åˆ›å»ºmockçš„LLMæœåŠ¡"""
    service = Mock()
    service.call_text_model = AsyncMock()
    return service


@pytest.fixture
def rule_engine(mock_llm_service):
    """åˆ›å»ºè§„åˆ™å¼•æ“å®ä¾‹"""
    return RuleEngine(llm_service=mock_llm_service)


@pytest.fixture
def sample_rule():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„è§„åˆ™"""
    now = datetime.now(UTC)
    return ETLRule(
        rule_id="test-rule-001",
        name="æµ‹è¯•è§„åˆ™",
        description="æå–æ ‡é¢˜å’Œæ‘˜è¦",
        json_schema={
            "type": "object",
            "properties": {
                "title": {"type": "string"},
                "summary": {"type": "string"}
            },
            "required": ["title", "summary"]
        },
        system_prompt="ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚",
        created_at=now,
        updated_at=now
    )


@pytest.fixture
def sample_markdown():
    """æµ‹è¯•ç”¨çš„Markdownå†…å®¹"""
    return """# æµ‹è¯•æ–‡æ¡£

è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£çš„å†…å®¹ã€‚

## ç¬¬ä¸€ç« èŠ‚

è¿™æ˜¯ç¬¬ä¸€ç« èŠ‚çš„å†…å®¹ã€‚

## ç¬¬äºŒç« èŠ‚

è¿™æ˜¯ç¬¬äºŒç« èŠ‚çš„å†…å®¹ã€‚
"""


# ============= Promptæ„é€ æµ‹è¯• =============


def test_build_prompt(rule_engine, sample_rule, sample_markdown):
    """æµ‹è¯•Promptæ„é€ """
    prompt = rule_engine._build_prompt(sample_markdown, sample_rule.json_schema)
    
    # éªŒè¯promptåŒ…å«å¿…è¦çš„å…ƒç´ 
    assert "JSON Schema" in prompt
    assert "Markdown Document" in prompt
    assert sample_markdown in prompt
    
    # éªŒè¯schemaè¢«æ­£ç¡®æ ¼å¼åŒ–
    schema_str = json.dumps(sample_rule.json_schema, indent=2)
    assert schema_str in prompt
    
    # éªŒè¯åŒ…å«æŒ‡å¯¼æ€§æ–‡å­—
    assert "extract" in prompt.lower() or "transform" in prompt.lower()
    assert "json" in prompt.lower()


def test_build_prompt_with_complex_schema(rule_engine, sample_markdown):
    """æµ‹è¯•å¤æ‚schemaçš„Promptæ„é€ """
    complex_schema = {
        "type": "object",
        "properties": {
            "title": {"type": "string"},
            "sections": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "heading": {"type": "string"},
                        "content": {"type": "string"}
                    }
                }
            },
            "metadata": {
                "type": "object",
                "properties": {
                    "author": {"type": "string"},
                    "date": {"type": "string"}
                }
            }
        }
    }
    
    prompt = rule_engine._build_prompt(sample_markdown, complex_schema)
    
    # éªŒè¯åµŒå¥—ç»“æ„è¢«æ­£ç¡®åŒ…å«
    assert "sections" in prompt
    assert "metadata" in prompt
    assert "array" in prompt


def test_build_prompt_with_empty_markdown(rule_engine, sample_rule):
    """æµ‹è¯•ç©ºMarkdownå†…å®¹çš„Promptæ„é€ """
    prompt = rule_engine._build_prompt("", sample_rule.json_schema)
    
    # å³ä½¿å†…å®¹ä¸ºç©ºï¼Œpromptç»“æ„åº”è¯¥å®Œæ•´
    assert "JSON Schema" in prompt
    assert "Markdown Document" in prompt


# ============= JSON SchemaéªŒè¯æµ‹è¯• =============


def test_validate_output_success(rule_engine, sample_rule):
    """æµ‹è¯•æˆåŠŸçš„JSONéªŒè¯"""
    valid_output = {
        "title": "æµ‹è¯•æ ‡é¢˜",
        "summary": "æµ‹è¯•æ‘˜è¦"
    }
    
    is_valid, error = rule_engine.validate_output(valid_output, sample_rule.json_schema)
    
    assert is_valid is True
    assert error is None


def test_validate_output_missing_required_field(rule_engine, sample_rule):
    """æµ‹è¯•ç¼ºå°‘å¿…éœ€å­—æ®µçš„éªŒè¯"""
    invalid_output = {
        "title": "æµ‹è¯•æ ‡é¢˜"
        # ç¼ºå°‘ summary
    }
    
    is_valid, error = rule_engine.validate_output(invalid_output, sample_rule.json_schema)
    
    assert is_valid is False
    assert error is not None
    assert "summary" in error.lower() or "required" in error.lower()


def test_validate_output_wrong_type(rule_engine, sample_rule):
    """æµ‹è¯•ç±»å‹é”™è¯¯çš„éªŒè¯"""
    invalid_output = {
        "title": 123,  # åº”è¯¥æ˜¯string
        "summary": "æµ‹è¯•æ‘˜è¦"
    }
    
    is_valid, error = rule_engine.validate_output(invalid_output, sample_rule.json_schema)
    
    assert is_valid is False
    assert error is not None


def test_validate_output_additional_properties(rule_engine):
    """æµ‹è¯•é¢å¤–å±æ€§çš„éªŒè¯"""
    schema = {
        "type": "object",
        "properties": {
            "title": {"type": "string"}
        },
        "additionalProperties": False
    }
    
    output_with_extra = {
        "title": "æµ‹è¯•",
        "extra_field": "ä¸åº”è¯¥å­˜åœ¨"
    }
    
    is_valid, error = rule_engine.validate_output(output_with_extra, schema)
    
    assert is_valid is False
    assert error is not None


def test_validate_output_nested_structure(rule_engine):
    """æµ‹è¯•åµŒå¥—ç»“æ„çš„éªŒè¯"""
    schema = {
        "type": "object",
        "properties": {
            "sections": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "heading": {"type": "string"},
                        "content": {"type": "string"}
                    },
                    "required": ["heading", "content"]
                }
            }
        }
    }
    
    valid_output = {
        "sections": [
            {"heading": "æ ‡é¢˜1", "content": "å†…å®¹1"},
            {"heading": "æ ‡é¢˜2", "content": "å†…å®¹2"}
        ]
    }
    
    is_valid, error = rule_engine.validate_output(valid_output, schema)
    
    assert is_valid is True
    assert error is None
    
    # æµ‹è¯•æ— æ•ˆçš„åµŒå¥—ç»“æ„
    invalid_output = {
        "sections": [
            {"heading": "æ ‡é¢˜1"}  # ç¼ºå°‘content
        ]
    }
    
    is_valid, error = rule_engine.validate_output(invalid_output, schema)
    
    assert is_valid is False
    assert error is not None


# ============= è§„åˆ™åº”ç”¨æµ‹è¯• =============


@pytest.mark.asyncio
async def test_apply_rule_success(rule_engine, mock_llm_service, sample_rule, sample_markdown):
    """æµ‹è¯•æˆåŠŸåº”ç”¨è§„åˆ™"""
    # Mock LLMå“åº”
    valid_json = {
        "title": "æµ‹è¯•æ–‡æ¡£",
        "summary": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£çš„æ‘˜è¦"
    }
    
    mock_llm_service.call_text_model.return_value = TextModelResponse(
        content=json.dumps(valid_json),
        model="gpt-4",
        usage={"prompt_tokens": 100, "completion_tokens": 50},
        finish_reason="stop"
    )
    
    # åº”ç”¨è§„åˆ™
    result = await rule_engine.apply_rule(sample_markdown, sample_rule)
    
    # éªŒè¯ç»“æœ
    assert result.success is True
    assert result.output == valid_json
    assert result.error is None
    assert result.llm_usage is not None
    
    # éªŒè¯LLMè¢«æ­£ç¡®è°ƒç”¨
    mock_llm_service.call_text_model.assert_called_once()
    call_args = mock_llm_service.call_text_model.call_args
    assert call_args.kwargs["system_prompt"] == sample_rule.system_prompt
    assert call_args.kwargs["response_format"] == "json_object"


@pytest.mark.asyncio
async def test_apply_rule_with_invalid_json(rule_engine, mock_llm_service, sample_rule, sample_markdown):
    """æµ‹è¯•LLMè¿”å›æ— æ•ˆJSONçš„é‡è¯•"""
    # ç¬¬ä¸€æ¬¡è¿”å›æ— æ•ˆJSONï¼Œç¬¬äºŒæ¬¡è¿”å›æœ‰æ•ˆJSON
    valid_json = {"title": "æµ‹è¯•", "summary": "æ‘˜è¦"}
    
    mock_llm_service.call_text_model.side_effect = [
        TextModelResponse(content="è¿™ä¸æ˜¯JSON", model="gpt-4", usage={}, finish_reason="stop"),
        TextModelResponse(content=json.dumps(valid_json), model="gpt-4", usage={}, finish_reason="stop")
    ]
    
    result = await rule_engine.apply_rule(sample_markdown, sample_rule, max_retries=2)
    
    # åº”è¯¥é‡è¯•å¹¶æœ€ç»ˆæˆåŠŸ
    assert result.success is True
    assert result.output == valid_json
    assert mock_llm_service.call_text_model.call_count == 2


@pytest.mark.asyncio
async def test_apply_rule_with_schema_validation_failure(rule_engine, mock_llm_service, sample_rule, sample_markdown):
    """æµ‹è¯•JSON SchemaéªŒè¯å¤±è´¥çš„é‡è¯•"""
    invalid_json = {"title": "æµ‹è¯•"}  # ç¼ºå°‘summary
    valid_json = {"title": "æµ‹è¯•", "summary": "æ‘˜è¦"}
    
    mock_llm_service.call_text_model.side_effect = [
        TextModelResponse(content=json.dumps(invalid_json), model="gpt-4", usage={}, finish_reason="stop"),
        TextModelResponse(content=json.dumps(valid_json), model="gpt-4", usage={}, finish_reason="stop")
    ]
    
    result = await rule_engine.apply_rule(sample_markdown, sample_rule, max_retries=2)
    
    # åº”è¯¥é‡è¯•å¹¶æœ€ç»ˆæˆåŠŸ
    assert result.success is True
    assert result.output == valid_json
    assert mock_llm_service.call_text_model.call_count == 2


@pytest.mark.asyncio
async def test_apply_rule_max_retries_exhausted(rule_engine, mock_llm_service, sample_rule, sample_markdown):
    """æµ‹è¯•é‡è¯•æ¬¡æ•°ç”¨å°½"""
    # æ¯æ¬¡éƒ½è¿”å›æ— æ•ˆJSON
    mock_llm_service.call_text_model.return_value = TextModelResponse(
        content="æ— æ•ˆçš„JSON",
        model="gpt-4",
        usage={},
        finish_reason="stop"
    )
    
    result = await rule_engine.apply_rule(sample_markdown, sample_rule, max_retries=2)
    
    # åº”è¯¥å¤±è´¥
    assert result.success is False
    assert result.output is None
    assert result.error is not None
    assert "Invalid JSON" in result.error or "json" in result.error.lower()
    
    # åº”è¯¥å°è¯•äº†3æ¬¡ï¼ˆåˆå§‹ + 2æ¬¡é‡è¯•ï¼‰
    assert mock_llm_service.call_text_model.call_count == 3


@pytest.mark.asyncio
async def test_apply_rule_llm_error(rule_engine, mock_llm_service, sample_rule, sample_markdown):
    """æµ‹è¯•LLMè°ƒç”¨é”™è¯¯"""
    # Mock LLMæŠ›å‡ºé”™è¯¯
    mock_llm_service.call_text_model.side_effect = LLMError("APIè°ƒç”¨å¤±è´¥")
    
    result = await rule_engine.apply_rule(sample_markdown, sample_rule)
    
    # åº”è¯¥è¿”å›é”™è¯¯ç»“æœ
    assert result.success is False
    assert result.output is None
    assert result.error is not None
    assert "LLM error" in result.error


@pytest.mark.asyncio
async def test_apply_rule_with_retry_feedback(rule_engine, mock_llm_service, sample_rule, sample_markdown):
    """æµ‹è¯•é‡è¯•æ—¶çš„é”™è¯¯åé¦ˆ"""
    invalid_json = {"title": "æµ‹è¯•"}  # ç¼ºå°‘summary
    valid_json = {"title": "æµ‹è¯•", "summary": "æ‘˜è¦"}
    
    mock_llm_service.call_text_model.side_effect = [
        TextModelResponse(content=json.dumps(invalid_json), model="gpt-4", usage={}, finish_reason="stop"),
        TextModelResponse(content=json.dumps(valid_json), model="gpt-4", usage={}, finish_reason="stop")
    ]
    
    result = await rule_engine.apply_rule(sample_markdown, sample_rule, max_retries=2)
    
    assert result.success is True
    
    # éªŒè¯ç¬¬äºŒæ¬¡è°ƒç”¨åŒ…å«é”™è¯¯åé¦ˆ
    second_call = mock_llm_service.call_text_model.call_args_list[1]
    prompt = second_call.kwargs["prompt"]
    assert "failed" in prompt.lower() or "error" in prompt.lower()


# ============= è¾¹ç•Œæƒ…å†µæµ‹è¯• =============


@pytest.mark.asyncio
async def test_apply_rule_with_empty_markdown(rule_engine, mock_llm_service, sample_rule):
    """æµ‹è¯•ç©ºMarkdownå†…å®¹"""
    valid_json = {"title": "ç©ºæ–‡æ¡£", "summary": "æ— å†…å®¹"}
    
    mock_llm_service.call_text_model.return_value = TextModelResponse(
        content=json.dumps(valid_json),
        model="gpt-4",
        usage={},
        finish_reason="stop"
    )
    
    result = await rule_engine.apply_rule("", sample_rule)
    
    assert result.success is True
    assert result.output == valid_json


@pytest.mark.asyncio
async def test_apply_rule_with_large_markdown(rule_engine, mock_llm_service, sample_rule):
    """æµ‹è¯•å¤§å‹Markdownå†…å®¹"""
    large_markdown = "# æ ‡é¢˜\n\n" + "æ®µè½å†…å®¹ã€‚\n\n" * 1000
    valid_json = {"title": "å¤§æ–‡æ¡£", "summary": "å¾ˆé•¿çš„å†…å®¹"}
    
    mock_llm_service.call_text_model.return_value = TextModelResponse(
        content=json.dumps(valid_json),
        model="gpt-4",
        usage={},
        finish_reason="stop"
    )
    
    result = await rule_engine.apply_rule(large_markdown, sample_rule)
    
    assert result.success is True
    
    # éªŒè¯å¤§å†…å®¹è¢«åŒ…å«åœ¨promptä¸­
    call_args = mock_llm_service.call_text_model.call_args
    assert large_markdown in call_args.kwargs["prompt"]


@pytest.mark.asyncio
async def test_apply_rule_with_unicode_content(rule_engine, mock_llm_service, sample_rule):
    """æµ‹è¯•åŒ…å«Unicodeå­—ç¬¦çš„å†…å®¹"""
    unicode_markdown = """# æµ‹è¯•æ–‡æ¡£ ğŸš€

è¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸­æ–‡ã€emojiå’Œå…¶ä»–Unicodeå­—ç¬¦çš„æ–‡æ¡£ï¼š
- ä¸­æ–‡ï¼šä½ å¥½ä¸–ç•Œ
- æ—¥æ–‡ï¼šã“ã‚“ã«ã¡ã¯
- emojiï¼šğŸ˜€ğŸ‰âœ¨
- ç‰¹æ®Šç¬¦å·ï¼šÂ©Â®â„¢
"""
    
    valid_json = {"title": "Unicodeæ–‡æ¡£", "summary": "åŒ…å«å¤šç§å­—ç¬¦"}
    
    mock_llm_service.call_text_model.return_value = TextModelResponse(
        content=json.dumps(valid_json, ensure_ascii=False),
        model="gpt-4",
        usage={},
        finish_reason="stop"
    )
    
    result = await rule_engine.apply_rule(unicode_markdown, sample_rule)
    
    assert result.success is True
    assert result.output == valid_json


@pytest.mark.asyncio
async def test_apply_rule_with_no_retries(rule_engine, mock_llm_service, sample_rule, sample_markdown):
    """æµ‹è¯•ä¸å…è®¸é‡è¯•ï¼ˆmax_retries=0ï¼‰"""
    invalid_json = {"title": "æµ‹è¯•"}  # ç¼ºå°‘summary
    
    mock_llm_service.call_text_model.return_value = TextModelResponse(
        content=json.dumps(invalid_json),
        model="gpt-4",
        usage={},
        finish_reason="stop"
    )
    
    result = await rule_engine.apply_rule(sample_markdown, sample_rule, max_retries=0)
    
    # åº”è¯¥ç«‹å³å¤±è´¥ï¼Œåªè°ƒç”¨ä¸€æ¬¡
    assert result.success is False
    assert mock_llm_service.call_text_model.call_count == 1


# ============= é›†æˆåœºæ™¯æµ‹è¯• =============


@pytest.mark.asyncio
async def test_apply_rule_complex_schema(rule_engine, mock_llm_service):
    """æµ‹è¯•å¤æ‚schemaçš„è§„åˆ™åº”ç”¨"""
    now = datetime.now(UTC)
    complex_rule = ETLRule(
        rule_id="complex-rule",
        name="å¤æ‚è§„åˆ™",
        description="æå–å¤æ‚ç»“æ„",
        json_schema={
            "type": "object",
            "properties": {
                "title": {"type": "string"},
                "sections": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "heading": {"type": "string"},
                            "content": {"type": "string"},
                            "subsections": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        },
                        "required": ["heading", "content"]
                    }
                }
            },
            "required": ["title", "sections"]
        },
        created_at=now,
        updated_at=now
    )
    
    valid_output = {
        "title": "å¤æ‚æ–‡æ¡£",
        "sections": [
            {
                "heading": "ç« èŠ‚1",
                "content": "å†…å®¹1",
                "subsections": ["å­ç« èŠ‚1.1", "å­ç« èŠ‚1.2"]
            },
            {
                "heading": "ç« èŠ‚2",
                "content": "å†…å®¹2",
                "subsections": []
            }
        ]
    }
    
    mock_llm_service.call_text_model.return_value = TextModelResponse(
        content=json.dumps(valid_output),
        model="gpt-4",
        usage={},
        finish_reason="stop"
    )
    
    result = await rule_engine.apply_rule("# æµ‹è¯•", complex_rule)
    
    assert result.success is True
    assert result.output == valid_output
    assert len(result.output["sections"]) == 2
    assert len(result.output["sections"][0]["subsections"]) == 2


if __name__ == "__main__":
    pytest.main([__file__, "-v"])

