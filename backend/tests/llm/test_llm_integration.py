"""LLM Service é›†æˆæµ‹è¯• - ä½¿ç”¨çœŸå®æ¨¡å‹ openrouter/qwen/qwen3-8b

æ­¤æµ‹è¯•æ–‡ä»¶ä»…è°ƒç”¨çœŸå®çš„ LLM æ¨¡å‹ï¼Œéœ€è¦è®¾ç½®æ­£ç¡®çš„ OPENROUTER_API_KEY ç¯å¢ƒå˜é‡ã€‚
æµ‹è¯•å°†éªŒè¯ LLM æœåŠ¡çš„æ ¸å¿ƒåŠŸèƒ½ã€‚
"""

import json
import os

import pytest

# é…ç½® litellm ä»¥é¿å… event loop å†²çª
import litellm
litellm.turn_off_message_logging = True  # ç¦ç”¨æ¶ˆæ¯æ—¥å¿—
litellm.suppress_debug_info = True  # ç¦ç”¨è°ƒè¯•ä¿¡æ¯

from src.llm.exceptions import (
    APIKeyError,
    InvalidResponseError,
    LLMError,
    ModelNotFoundError,
)
from src.llm.schemas import TextModelRequest, TextModelResponse
from src.llm.service import LLMService

# æµ‹è¯•ä½¿ç”¨çš„æ¨¡å‹
TEST_MODEL = "openrouter/qwen/qwen3-8b"


@pytest.fixture
def llm_service():
    """åˆ›å»º LLM æœåŠ¡å®ä¾‹"""
    service = LLMService()
    return service


@pytest.fixture(autouse=True)
def check_api_key():
    """æ£€æŸ¥ API å¯†é’¥æ˜¯å¦å·²è®¾ç½®"""
    if not os.environ.get("OPENROUTER_API_KEY"):
        pytest.skip("éœ€è¦è®¾ç½® OPENROUTER_API_KEY ç¯å¢ƒå˜é‡æ‰èƒ½è¿è¡Œé›†æˆæµ‹è¯•")


# ============= åŸºç¡€åŠŸèƒ½æµ‹è¯• =============


@pytest.mark.asyncio
async def test_service_initialization(llm_service):
    """æµ‹è¯•æœåŠ¡åˆå§‹åŒ–"""
    assert llm_service is not None
    assert llm_service.default_model is not None
    assert len(llm_service.supported_models) > 0
    assert TEST_MODEL in llm_service.supported_models


@pytest.mark.asyncio
async def test_get_supported_models(llm_service):
    """æµ‹è¯•è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
    models = llm_service.get_supported_models()
    assert isinstance(models, list)
    assert len(models) > 0
    assert TEST_MODEL in models


@pytest.mark.asyncio
async def test_is_model_supported(llm_service):
    """æµ‹è¯•æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ"""
    assert llm_service.is_model_supported(TEST_MODEL) is True
    assert llm_service.is_model_supported("nonexistent/model") is False


# ============= æ–‡æœ¬ç”Ÿæˆæµ‹è¯• =============


@pytest.mark.asyncio
async def test_simple_text_generation(llm_service):
    """æµ‹è¯•ç®€å•çš„æ–‡æœ¬ç”Ÿæˆ"""
    prompt = "What is artificial intelligence? Please answer in one sentence."

    response = await llm_service.call_text_model(
        system_prompt="/no_think",
        prompt=prompt,
        model=TEST_MODEL,
        temperature=0.7,
        max_tokens=200,
    )

    # éªŒè¯å“åº”ç»“æ„
    assert isinstance(response, TextModelResponse)
    assert response.content is not None
    assert response.model == TEST_MODEL
    assert response.finish_reason in ["stop", "length", "eos", "end_turn"]

    # éªŒè¯ token ä½¿ç”¨ç»Ÿè®¡
    assert "prompt_tokens" in response.usage
    assert "completion_tokens" in response.usage
    assert "total_tokens" in response.usage
    assert response.usage["prompt_tokens"] > 0
    assert response.usage["completion_tokens"] > 0
    assert (
        response.usage["total_tokens"]
        == response.usage["prompt_tokens"] + response.usage["completion_tokens"]
    )

    print(f"\nç”Ÿæˆçš„å†…å®¹: {response.content}")
    print(f"Content length: {len(response.content)}")
    print(f"Token ä½¿ç”¨: {response.usage}")
    
    # å¦‚æœå†…å®¹ä¸ä¸ºç©ºï¼ŒéªŒè¯å…¶é•¿åº¦
    if response.content:
        assert len(response.content) > 0


@pytest.mark.asyncio
async def test_text_generation_with_system_prompt(llm_service):
    """æµ‹è¯•å¸¦ç³»ç»Ÿæç¤ºçš„æ–‡æœ¬ç”Ÿæˆ"""
    system_prompt = "/no_think \n\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£å†™ä½œåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"
    prompt = "ä»€ä¹ˆæ˜¯ RESTful APIï¼Ÿ"

    response = await llm_service.call_text_model(
        prompt=prompt,
        system_prompt=system_prompt,
        model=TEST_MODEL,
        temperature=0.3,
        max_tokens=200,
    )

    assert isinstance(response, TextModelResponse)
    assert response.content is not None
    assert len(response.content) > 0
    assert response.model == TEST_MODEL
    assert response.usage["total_tokens"] > 0

    print(f"\nç”Ÿæˆçš„å†…å®¹: {response.content}")


@pytest.mark.asyncio
async def test_text_generation_with_low_temperature(llm_service):
    """æµ‹è¯•ä½æ¸©åº¦å‚æ•°çš„æ–‡æœ¬ç”Ÿæˆï¼ˆæ›´ç¡®å®šæ€§ï¼‰"""
    prompt = "1 + 1 ç­‰äºå¤šå°‘ï¼Ÿè¯·åªå›ç­”æ•°å­—ã€‚"

    response = await llm_service.call_text_model(
        system_prompt="/no_think",
        prompt=prompt,
        model=TEST_MODEL,
        temperature=0.1,  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§
        max_tokens=10,
    )

    assert isinstance(response, TextModelResponse)
    assert response.content is not None
    # åº”è¯¥åŒ…å«æ•°å­— 2
    assert "2" in response.content

    print(f"\nç”Ÿæˆçš„å†…å®¹: {response.content}")


# ============= JSON æ¨¡å¼æµ‹è¯• =============


@pytest.mark.asyncio
async def test_json_object_generation(llm_service):
    """æµ‹è¯• JSON å¯¹è±¡ç”Ÿæˆ"""
    system_prompt = "/no_think \n\nä½ æ˜¯ä¸€ä¸ª JSON ç”Ÿæˆå™¨ï¼Œåªè¿”å›æœ‰æ•ˆçš„ JSON å¯¹è±¡ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚"
    prompt = """
è¯·ç”Ÿæˆä¸€ä¸ªæè¿°ä¸€æœ¬ä¹¦çš„ JSON å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- title: ä¹¦å
- author: ä½œè€…
- year: å‡ºç‰ˆå¹´ä»½
- genre: ç±»å‹

è¯·ç”Ÿæˆä¸€ä¸ªç§‘å¹»å°è¯´çš„ä¾‹å­ã€‚
"""

    response = await llm_service.call_text_model(
        prompt=prompt,
        system_prompt=system_prompt,
        model=TEST_MODEL,
        temperature=0.3,
        response_format="json_object",
        max_tokens=300,
    )

    assert isinstance(response, TextModelResponse)
    assert response.content is not None
    assert response.model == TEST_MODEL

    # éªŒè¯è¿”å›çš„å†…å®¹æ˜¯æœ‰æ•ˆçš„ JSON
    try:
        data = json.loads(response.content)
        print(f"\nç”Ÿæˆçš„ JSON: {json.dumps(data, ensure_ascii=False, indent=2)}")

        # å¯é€‰ï¼šéªŒè¯ JSON ç»“æ„ï¼ˆå¦‚æœæ¨¡å‹æ­£ç¡®ç†è§£äº†æŒ‡ä»¤ï¼‰
        # æ³¨æ„ï¼šç”±äºæ¨¡å‹å¯èƒ½ä¸å®Œå…¨éµå¾ªæŒ‡ä»¤ï¼Œè¿™éƒ¨åˆ†éªŒè¯å¯èƒ½ä¼šå¤±è´¥
        # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…æµ‹è¯•å¯ä»¥æ›´å®½æ¾
        if isinstance(data, dict):
            print("âœ“ JSON å¯¹è±¡ç»“æ„æ­£ç¡®")
    except json.JSONDecodeError:
        pytest.fail(f"æ¨¡å‹è¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSON: {response.content}")


@pytest.mark.asyncio
async def test_json_array_generation(llm_service):
    """æµ‹è¯• JSON æ•°ç»„ç”Ÿæˆ"""
    system_prompt = "/no_think \n\nä½ æ˜¯ä¸€ä¸ª JSON ç”Ÿæˆå™¨ï¼Œåªè¿”å›æœ‰æ•ˆçš„ JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚"
    prompt = """
è¯·ç”Ÿæˆä¸€ä¸ªåŒ…å« 3 ä¸ªæ°´æœå¯¹è±¡çš„ JSON æ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡æœ‰ name å’Œ color å­—æ®µã€‚
"""

    response = await llm_service.call_text_model(
        prompt=prompt,
        system_prompt=system_prompt,
        model=TEST_MODEL,
        temperature=0.3,
        response_format="json_object",
        max_tokens=300,
    )

    assert isinstance(response, TextModelResponse)
    assert response.content is not None

    # éªŒè¯æ˜¯æœ‰æ•ˆçš„ JSON
    try:
        data = json.loads(response.content)
        print(f"\nç”Ÿæˆçš„ JSON: {json.dumps(data, ensure_ascii=False, indent=2)}")
    except json.JSONDecodeError:
        pytest.fail(f"æ¨¡å‹è¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSON: {response.content}")


# ============= Request å¯¹è±¡æµ‹è¯• =============


@pytest.mark.asyncio
async def test_call_with_request_object(llm_service):
    """æµ‹è¯•ä½¿ç”¨ Request å¯¹è±¡è°ƒç”¨æ¨¡å‹"""
    request = TextModelRequest(
        prompt="è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚",
        system_prompt="/no_think \n\nä½ æ˜¯ä¸€ä¸ª AI æ•™è‚²ä¸“å®¶ã€‚",
        model=TEST_MODEL,
        temperature=0.5,
        response_format="text",
        max_tokens=150,
    )

    response = await llm_service.call_text_model_from_request(request)

    assert isinstance(response, TextModelResponse)
    assert response.content is not None
    assert len(response.content) > 0
    assert response.model == TEST_MODEL
    assert response.usage["total_tokens"] > 0

    print(f"\nç”Ÿæˆçš„å†…å®¹: {response.content}")


# ============= é”™è¯¯å¤„ç†æµ‹è¯• =============


@pytest.mark.asyncio
async def test_unsupported_model_error(llm_service):
    """æµ‹è¯•ä¸æ”¯æŒçš„æ¨¡å‹é”™è¯¯"""
    with pytest.raises(ModelNotFoundError) as exc_info:
        await llm_service.call_text_model(
            system_prompt="/no_think",
            prompt="Test prompt",
            model="nonexistent/model",
        )

    error = exc_info.value
    assert error.model == "nonexistent/model"
    assert len(error.available_models) > 0
    assert "not supported" in str(error)


@pytest.mark.asyncio
async def test_empty_prompt_handling(llm_service):
    """æµ‹è¯•ç©ºæç¤ºçš„å¤„ç†"""
    # å³ä½¿æç¤ºä¸ºç©ºï¼Œä¹Ÿåº”è¯¥èƒ½å¤Ÿè°ƒç”¨æ¨¡å‹ï¼ˆæ¨¡å‹å¯èƒ½è¿”å›ç©ºæˆ–é»˜è®¤å“åº”ï¼‰
    try:
        response = await llm_service.call_text_model(
            prompt="",
            system_prompt="/no_think",
            model=TEST_MODEL,
            max_tokens=50,
        )

        # å¦‚æœæˆåŠŸï¼ŒéªŒè¯å“åº”ç»“æ„
        assert isinstance(response, TextModelResponse)
        print(f"\nç©ºæç¤ºçš„å“åº”: {response.content}")
    except Exception as e:
        # æŸäº›æ¨¡å‹å¯èƒ½ä¸æ¥å—ç©ºæç¤º
        print(f"\nç©ºæç¤ºå¯¼è‡´é”™è¯¯ï¼ˆé¢„æœŸè¡Œä¸ºï¼‰: {e}")


# ============= è¾¹ç•Œæ¡ä»¶æµ‹è¯• =============


@pytest.mark.asyncio
async def test_long_prompt_handling(llm_service):
    """æµ‹è¯•é•¿æç¤ºçš„å¤„ç†"""
    # åˆ›å»ºä¸€ä¸ªè¾ƒé•¿çš„æç¤º
    long_prompt = "è¯·æ€»ç»“ä»¥ä¸‹æ–‡æœ¬: " + "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ã€‚" * 50

    response = await llm_service.call_text_model(
        prompt=long_prompt,
        model=TEST_MODEL,
        temperature=0.3,
        max_tokens=200,
    )

    assert isinstance(response, TextModelResponse)
    assert response.content is not None
    assert response.usage["prompt_tokens"] > 100  # åº”è¯¥æœ‰è¾ƒå¤šçš„è¾“å…¥ token

    print(f"\né•¿æç¤ºçš„ Token ä½¿ç”¨: {response.usage}")


@pytest.mark.asyncio
async def test_max_tokens_limit(llm_service):
    """æµ‹è¯• max_tokens é™åˆ¶"""
    response = await llm_service.call_text_model(
        prompt="è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„é•¿æ–‡ç« ã€‚",
        model=TEST_MODEL,
        temperature=0.7,
        max_tokens=50,  # é™åˆ¶è¾“å‡ºé•¿åº¦
    )

    assert isinstance(response, TextModelResponse)
    assert response.usage["completion_tokens"] <= 50
    # å¯èƒ½å› ä¸ºè¾¾åˆ° token é™åˆ¶è€Œç»“æŸ
    print(f"\nFinish reason: {response.finish_reason}")
    print(f"Completion tokens: {response.usage['completion_tokens']}")


# ============= å¤šæ¬¡è°ƒç”¨ç¨³å®šæ€§æµ‹è¯• =============


@pytest.mark.asyncio
async def test_multiple_sequential_calls(llm_service):
    """æµ‹è¯•è¿ç»­å¤šæ¬¡è°ƒç”¨çš„ç¨³å®šæ€§"""
    prompts = [
        "1+1=?",
        "2+2=?",
        "3+3=?",
    ]

    responses = []
    for prompt in prompts:
        response = await llm_service.call_text_model(
            prompt=prompt,
            model=TEST_MODEL,
            temperature=0.1,
            max_tokens=20,
        )
        responses.append(response)

    # éªŒè¯æ‰€æœ‰è°ƒç”¨éƒ½æˆåŠŸ
    assert len(responses) == 3
    for i, response in enumerate(responses):
        assert isinstance(response, TextModelResponse)
        assert response.content is not None
        print(f"\nç¬¬ {i+1} æ¬¡è°ƒç”¨ç»“æœ: {response.content}")


@pytest.mark.asyncio
async def test_concurrent_calls(llm_service):
    """æµ‹è¯•å¹¶å‘è°ƒç”¨çš„ç¨³å®šæ€§"""
    import asyncio

    prompts = [
        "ä»€ä¹ˆæ˜¯ Python?",
        "ä»€ä¹ˆæ˜¯ JavaScript?",
        "ä»€ä¹ˆæ˜¯ Java?",
    ]

    # å¹¶å‘è°ƒç”¨
    tasks = [
        llm_service.call_text_model(
            prompt=prompt,
            model=TEST_MODEL,
            temperature=0.3,
            max_tokens=100,
        )
        for prompt in prompts
    ]

    responses = await asyncio.gather(*tasks)

    # éªŒè¯æ‰€æœ‰è°ƒç”¨éƒ½æˆåŠŸ
    assert len(responses) == 3
    for i, response in enumerate(responses):
        assert isinstance(response, TextModelResponse)
        assert response.content is not None
        assert response.model == TEST_MODEL
        print(f"\nå¹¶å‘è°ƒç”¨ {i+1} ç»“æœ: {response.content[:50]}...")


# ============= ç‰¹æ®Šå­—ç¬¦å’Œç¼–ç æµ‹è¯• =============


@pytest.mark.asyncio
async def test_unicode_handling(llm_service):
    """æµ‹è¯• Unicode å­—ç¬¦å¤„ç†"""
    prompt = "è¯·ç”¨ä¸€å¥è¯ä»‹ç»: æ—¥æœ¬èªã€í•œêµ­ì–´ã€ä¸­æ–‡ã€Emoji ğŸ˜€ğŸ‰ğŸš€"

    response = await llm_service.call_text_model(
        prompt=prompt,
        model=TEST_MODEL,
        temperature=0.5,
        max_tokens=150,
    )

    assert isinstance(response, TextModelResponse)
    assert response.content is not None
    print(f"\nUnicode å“åº”: {response.content}")


@pytest.mark.asyncio
async def test_special_characters_handling(llm_service):
    """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†"""
    prompt = "è¯·è§£é‡Šè¿™äº›ç¬¦å·: @#$%^&*()[]{}|\\/<>?~`"

    response = await llm_service.call_text_model(
        prompt=prompt,
        model=TEST_MODEL,
        temperature=0.5,
        max_tokens=150,
    )

    assert isinstance(response, TextModelResponse)
    assert response.content is not None
    print(f"\nç‰¹æ®Šå­—ç¬¦å“åº”: {response.content}")


# ============= æ€§èƒ½å’Œå¥å£®æ€§æµ‹è¯• =============


@pytest.mark.asyncio
async def test_response_time(llm_service):
    """æµ‹è¯•å“åº”æ—¶é—´"""
    import time

    start_time = time.time()

    response = await llm_service.call_text_model(
        prompt="Hello, how are you?",
        model=TEST_MODEL,
        temperature=0.5,
        max_tokens=50,
    )

    end_time = time.time()
    elapsed_time = end_time - start_time

    assert isinstance(response, TextModelResponse)
    print(f"\nå“åº”æ—¶é—´: {elapsed_time:.2f} ç§’")

    # åˆç†çš„å“åº”æ—¶é—´åº”è¯¥åœ¨ 60 ç§’å†…ï¼ˆé…ç½®çš„è¶…æ—¶æ—¶é—´ï¼‰
    assert elapsed_time < 60


@pytest.mark.asyncio
async def test_deterministic_output_with_low_temperature(llm_service):
    """æµ‹è¯•ä½æ¸©åº¦ä¸‹çš„ç¡®å®šæ€§è¾“å‡º
    
    æ³¨æ„: Qwen3-8b æ˜¯ä¸€ä¸ªæ”¯æŒæ¨ç†æ¨¡å¼çš„æ¨¡å‹,ä¼šåœ¨ reasoning_content ä¸­æ”¾ç½®æ€è€ƒè¿‡ç¨‹,
    åœ¨ content ä¸­æ”¾ç½®æœ€ç»ˆç­”æ¡ˆã€‚éœ€è¦è¶³å¤Ÿçš„ tokens æ‰èƒ½ç”Ÿæˆå®Œæ•´å“åº”ã€‚
    """
    # ä½¿ç”¨ç®€å•çš„é—®é¢˜,éœ€è¦è¶³å¤Ÿçš„ tokens è®©æ¨¡å‹å®Œæˆæ¨ç†å’Œç­”æ¡ˆ
    prompt = "What is 2+2? Answer with only the number."
    
    # ä½¿ç”¨ç›¸åŒå‚æ•°è°ƒç”¨ä¸¤æ¬¡,ç»™äºˆå……è¶³çš„ tokens
    response1 = await llm_service.call_text_model(
        prompt=prompt,
        model=TEST_MODEL,
        temperature=0.0,  # æœ€ä½æ¸©åº¦ï¼Œåº”è¯¥æœ€ç¡®å®š
        max_tokens=200,  # è¶³å¤Ÿçš„ tokens è®©æ¨¡å‹å®Œæˆæ¨ç†å’Œç­”æ¡ˆ
    )

    response2 = await llm_service.call_text_model(
        prompt=prompt,
        model=TEST_MODEL,
        temperature=0.0,
        max_tokens=200,
    )

    print(f"\nç¬¬ä¸€æ¬¡å“åº”: {response1.content}")
    print(f"ç¬¬äºŒæ¬¡å“åº”: {response2.content}")
    print(f"ç¬¬ä¸€æ¬¡ finish_reason: {response1.finish_reason}")
    print(f"ç¬¬äºŒæ¬¡ finish_reason: {response2.finish_reason}")

    # éªŒè¯å“åº”ä¸ä¸ºç©º
    assert len(response1.content.strip()) > 0, "ç¬¬ä¸€æ¬¡å“åº”å†…å®¹ä¸ºç©º"
    assert len(response2.content.strip()) > 0, "ç¬¬äºŒæ¬¡å“åº”å†…å®¹ä¸ºç©º"
    
    # éªŒè¯ä¸¤æ¬¡å“åº”éƒ½åŒ…å«ç­”æ¡ˆ "4"
    assert "4" in response1.content, f"ç¬¬ä¸€æ¬¡å“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆ: {response1.content}"
    assert "4" in response2.content, f"ç¬¬äºŒæ¬¡å“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆ: {response2.content}"

