"""
Agent Service - ç®€åŒ–ç‰ˆ

å‰ç«¯åªéœ€è¦ä¼  active_tool_idsï¼Œåç«¯è‡ªåŠ¨å¤„ç†ä¸€åˆ‡ï¼š
1. æ ¹æ® tool_id æŸ¥åº“è·å– tool é…ç½®
2. å¦‚æœæ˜¯ bash toolï¼Œè‡ªåŠ¨æŸ¥è¡¨æ•°æ®ã€å¯åŠ¨æ²™ç›’
3. å¦‚æœæ˜¯ search toolï¼Œæ³¨å†Œåˆ° Claude å¹¶ç›´æ¥è°ƒç”¨ SearchService
4. æ„å»º Claude è¯·æ±‚

æ”¯æŒçš„èŠ‚ç‚¹ç±»å‹ï¼š
- folder: é€’å½’è·å–å­æ–‡ä»¶ï¼Œåœ¨æ²™ç›’ä¸­é‡å»ºç›®å½•ç»“æ„
- json: å¯¼å‡º content ä¸º data.jsonï¼ˆå¯é€‰ json_path æå–å­æ•°æ®ï¼‰
- file/pdf/image/etc: ä¸‹è½½å•ä¸ªæ–‡ä»¶

æ”¯æŒçš„å·¥å…·ç±»å‹ï¼š
- bash (sandbox): é€šè¿‡ agent_bash é…ç½®ï¼Œæ•°æ®æ²™ç›’æ‰§è¡Œ
- search: é€šè¿‡ agent_tool å…³è”ï¼Œå‘é‡æ£€ç´¢ï¼ˆTurbopufferï¼‰
"""
import json
import re
import time
from dataclasses import dataclass, field
from typing import Any, AsyncGenerator, Iterable, Optional

from loguru import logger

from src.agent.schemas import AgentRequest
from src.config import settings
from src.agent.chat.service import ChatService
from src.agent.config.service import AgentConfigService
from src.analytics.service import log_context_access, log_bash_execution
import time as time_module  # For latency tracking

# Anthropic å®˜æ–¹ bash å·¥å…·ï¼ˆComputer Use æ ¼å¼ï¼Œä»…å®˜æ–¹ API æ”¯æŒï¼‰
BASH_TOOL_NATIVE = {"type": "bash_20250124", "name": "bash"}

# é€šç”¨ bash å·¥å…·å®šä¹‰ï¼ˆå…¼å®¹ç¬¬ä¸‰æ–¹ä»£ç†ç½‘å…³ï¼‰
BASH_TOOL_COMPAT = {
    "name": "bash",
    "description": (
        "Execute a bash command in the sandbox environment. "
        "Use this to run shell commands, view files, manipulate data, etc. "
        "Returns the command output (stdout and stderr combined)."
    ),
    "input_schema": {
        "type": "object",
        "properties": {
            "command": {
                "type": "string",
                "description": "The bash command to execute",
            }
        },
        "required": ["command"],
    },
}


def _use_native_anthropic() -> bool:
    """åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ Anthropic å®˜æ–¹ APIï¼ˆéç¬¬ä¸‰æ–¹ä»£ç†ï¼‰"""
    base_url = settings.ANTHROPIC_BASE_URL
    if not base_url:
        return True
    return "api.anthropic.com" in base_url


def _get_bash_tool() -> dict:
    """æ ¹æ® API ç«¯ç‚¹é€‰æ‹©åˆé€‚çš„ bash tool å®šä¹‰"""
    if _use_native_anthropic():
        return BASH_TOOL_NATIVE
    return BASH_TOOL_COMPAT


def _sanitize_tool_name(name: str) -> str:
    """å°†å·¥å…·åç§°è½¬æ¢ä¸º Claude å…¼å®¹çš„æ ¼å¼ (ä»… a-zA-Z0-9_-)"""
    sanitized = re.sub(r"[^a-zA-Z0-9_-]", "_", name)
    # å»æ‰é¦–å°¾çš„ä¸‹åˆ’çº¿å¹¶å»é‡è¿ç»­ä¸‹åˆ’çº¿
    sanitized = re.sub(r"_+", "_", sanitized).strip("_")
    return sanitized or "unnamed"


@dataclass
class SearchToolConfig:
    """Agent å…³è”çš„ Search Tool é…ç½®"""
    tool_id: str           # tool è¡¨çš„ ID
    node_id: str           # ç»‘å®šçš„ content_node ID
    json_path: str         # JSON è·¯å¾„
    project_id: str        # é¡¹ç›® ID
    node_type: str         # èŠ‚ç‚¹ç±»å‹ï¼ˆfolder / json / markdown ç­‰ï¼‰
    name: str              # å·¥å…·åŸå§‹åç§°
    description: str       # å·¥å…·æè¿°
    claude_tool_name: str  # æ³¨å†Œåˆ° Claude çš„å·¥å…·åç§°


@dataclass
class SandboxFile:
    """æ²™ç›’ä¸­çš„æ–‡ä»¶"""
    path: str               # æ²™ç›’ä¸­çš„è·¯å¾„ï¼Œå¦‚ /workspace/data.json
    content: str | None = None      # æ–‡æœ¬å†…å®¹ï¼ˆJSON/æ–‡æœ¬æ–‡ä»¶ï¼‰
    s3_key: str | None = None       # S3 keyï¼ˆäºŒè¿›åˆ¶æ–‡ä»¶éœ€è¦ä¸‹è½½ï¼‰
    content_type: str = "application/octet-stream"


@dataclass
class SandboxData:
    """æ²™ç›’æ•°æ®"""
    files: list[SandboxFile] = field(default_factory=list)
    node_type: str = "json"
    root_node_id: str = ""
    root_node_name: str = ""
    # ç”¨äºå¤šæ–‡ä»¶å›å†™ï¼šnode_id -> {sandbox_path, node_type}
    node_path_map: dict = field(default_factory=dict)


class AgentService:
    """Agent æ ¸å¿ƒé€»è¾‘"""

    def __init__(self, anthropic_client=None):
        self._anthropic = anthropic_client or _default_anthropic_client()

    async def execute_task_sync(
        self,
        agent_id: str,
        task_content: str,
        user_id: str,
        node_service,
        sandbox_service,
        s3_service=None,
        agent_config_service=None,
        max_iterations: int = 15,
    ) -> dict:
        """
        éæµå¼æ‰§è¡Œ Agent ä»»åŠ¡ï¼ˆç”¨äº Schedule Agentï¼‰
        
        å¤ç”¨ stream_events çš„æ ¸å¿ƒé€»è¾‘ï¼Œä½†ä¸éœ€è¦æµå¼è¾“å‡ºå’ŒèŠå¤©å†å²ã€‚
        
        Args:
            agent_id: Agent ID
            task_content: ä»»åŠ¡å†…å®¹ï¼ˆæ¥è‡ª agent.task_contentï¼‰
            user_id: ç”¨æˆ· IDï¼ˆagent çš„ ownerï¼‰
            node_service: ContentNodeService
            sandbox_service: SandboxService
            s3_service: S3Serviceï¼ˆå¯é€‰ï¼‰
            agent_config_service: AgentConfigService
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            
        Returns:
            dict with execution results
        """
        import time
        from src.config import settings
        
        result = {
            "status": "success",
            "output_summary": "",
            "tool_calls": [],
            "updated_nodes": [],
        }
        
        try:
            # ========== 1. è·å– Agent é…ç½® ==========
            if not agent_config_service:
                return {"status": "failed", "error": "agent_config_service is required"}
            
            agent = agent_config_service.get_agent(agent_id)
            if not agent:
                return {"status": "failed", "error": f"Agent not found: {agent_id}"}
            
            # é€šè¿‡ project éªŒè¯æƒé™
            if not agent_config_service.verify_access(agent_id, user_id):
                return {"status": "failed", "error": "Unauthorized access to agent"}
            
            logger.info(f"[ScheduleAgent] Executing agent: {agent.name} (id={agent_id})")
            
            # ========== 2. æ”¶é›† bash tools ==========
            bash_tools: list[dict] = []
            for access in agent.accesses:
                if access.terminal:
                    bash_tools.append({
                        "node_id": access.node_id,
                        "json_path": (access.json_path or "").strip(),
                        "readonly": access.terminal_readonly,
                    })
                    logger.info(f"[ScheduleAgent] Found bash access: node_id={access.node_id}")
            
            use_bash = len(bash_tools) > 0
            sandbox_readonly = all(tool["readonly"] for tool in bash_tools) if bash_tools else True
            
            # ========== 3. å‡†å¤‡æ²™ç›’æ•°æ® ==========
            sandbox_data: SandboxData | None = None
            sandbox_session_id = None
            node_path_map: dict = {}
            
            if use_bash and node_service:
                all_files: list[SandboxFile] = []
                primary_node_type = "folder"
                primary_node_id = ""
                primary_node_name = ""
                
                for i, tool in enumerate(bash_tools):
                    try:
                        data = await prepare_sandbox_data(
                            node_service=node_service,
                            node_id=tool["node_id"],
                            json_path=tool["json_path"],
                            user_id=user_id,
                        )
                        logger.info(f"[ScheduleAgent] Prepared sandbox data: node_id={tool['node_id']}, files={len(data.files)}")
                        all_files.extend(data.files)
                        
                        if data.files:
                            main_path = data.files[0].path
                            node_path_map[tool["node_id"]] = {
                                "path": main_path,
                                "node_type": data.node_type,
                                "json_path": tool["json_path"],
                                "readonly": tool["readonly"],
                            }
                        
                        if i == 0:
                            primary_node_type = data.node_type
                            primary_node_id = data.root_node_id
                            primary_node_name = data.root_node_name
                    except Exception as e:
                        logger.warning(f"[ScheduleAgent] Failed to prepare sandbox data: {e}")
                
                sandbox_data = SandboxData(
                    files=all_files,
                    node_type=primary_node_type if len(bash_tools) == 1 else "multi",
                    root_node_id=primary_node_id,
                    root_node_name=primary_node_name,
                    node_path_map=node_path_map,
                )
            
            # ========== 4. å¯åŠ¨æ²™ç›’ ==========
            if use_bash and sandbox_service:
                sandbox_session_id = f"schedule-{int(time.time() * 1000)}"
                
                if sandbox_data and sandbox_data.node_type == "json" and len(bash_tools) == 1:
                    json_content = {}
                    if sandbox_data.files:
                        try:
                            json_content = json.loads(sandbox_data.files[0].content or "{}")
                        except:
                            json_content = {}
                    start_result = await sandbox_service.start(
                        session_id=sandbox_session_id,
                        data=json_content,
                        readonly=sandbox_readonly,
                    )
                else:
                    start_result = await sandbox_service.start_with_files(
                        session_id=sandbox_session_id,
                        files=sandbox_data.files if sandbox_data else [],
                        readonly=sandbox_readonly,
                        s3_service=s3_service,
                    )
                
                if not start_result.get("success"):
                    return {"status": "failed", "error": start_result.get("error", "Failed to start sandbox")}
                
                logger.info(f"[ScheduleAgent] Sandbox started: {sandbox_session_id}")
            
            # ========== 5. æ„å»º Claude è¯·æ±‚ ==========
            tools: list[dict[str, Any]] = [_get_bash_tool()] if use_bash else []
            
            if use_bash and sandbox_data:
                node_type = sandbox_data.node_type
                if node_type == "json":
                    system_prompt = "You are an AI agent specialized in JSON data processing. Use the bash tool to view and manipulate data in /workspace/data.json."
                elif node_type == "folder":
                    system_prompt = "You are an AI agent with access to a file system. Use the bash tool to explore and manipulate files in /workspace/."
                elif node_type == "multi":
                    system_prompt = "You are an AI agent with access to multiple files and folders. Use the bash tool to explore and manipulate files in /workspace/."
                else:
                    system_prompt = f"You are an AI agent with access to a {node_type} file. Use the bash tool to analyze the file in /workspace/."
            else:
                system_prompt = "You are an AI agent, a helpful assistant."
            
            # æ„å»ºç”¨æˆ·æ¶ˆæ¯
            user_content = task_content
            if use_bash and sandbox_data:
                mode_str = "âš ï¸ åªè¯»æ¨¡å¼" if sandbox_readonly else "âœï¸ å¯è¯»å†™æ¨¡å¼"
                context_prefix = (
                    f"[æ•°æ®ä¸Šä¸‹æ–‡]\n"
                    f"å·¥ä½œç›®å½•: /workspace/\n"
                    f"æ–‡ä»¶æ•°é‡: {len(sandbox_data.files)}\n"
                    f"æƒé™: {mode_str}\n"
                    f"[ä»»åŠ¡]\n"
                )
                user_content = context_prefix + task_content
            
            messages = [{"role": "user", "content": user_content}]
            
            # ========== 6. è°ƒç”¨ Claude (éæµå¼å¾ªç¯) ==========
            iterations = 0
            all_text_outputs = []
            
            while iterations < max_iterations:
                iterations += 1
                logger.info(f"[ScheduleAgent] Claude iteration {iterations}")
                
                try:
                    # æ„å»º API è°ƒç”¨å‚æ•°ï¼ˆtools ä¸ºç©ºæ—¶ä¸ä¼ é€’ï¼Œé¿å…ä»£ç†ç½‘å…³å…¼å®¹é—®é¢˜ï¼‰
                    create_kwargs: dict[str, Any] = {
                        "model": settings.ANTHROPIC_MODEL,
                        "max_tokens": 4096,
                        "system": system_prompt,
                        "messages": messages,
                    }
                    if tools:
                        create_kwargs["tools"] = tools
                    
                    response = await self._anthropic.messages.create(**create_kwargs)
                    
                    stop_reason = response.stop_reason
                    content_blocks = response.content
                    
                    # å¤„ç†å“åº”å†…å®¹
                    tool_uses = []
                    response_content = []
                    
                    for block in content_blocks:
                        block_type = getattr(block, "type", None)
                        if block_type == "text":
                            text = getattr(block, "text", "")
                            all_text_outputs.append(text)
                            response_content.append({"type": "text", "text": text})
                        elif block_type == "tool_use":
                            tool_uses.append({
                                "id": getattr(block, "id", ""),
                                "name": getattr(block, "name", ""),
                                "input": getattr(block, "input", {}),
                            })
                            response_content.append({
                                "type": "tool_use",
                                "id": getattr(block, "id", ""),
                                "name": getattr(block, "name", ""),
                                "input": getattr(block, "input", {}),
                            })
                    
                    logger.info(f"[ScheduleAgent] Claude response: stop_reason={stop_reason}, tool_uses={len(tool_uses)}")
                    
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸ
                    if not tool_uses:
                        break
                    
                    # æ‰§è¡Œå·¥å…·
                    tool_results = []
                    for tool in tool_uses:
                        tool_name = tool.get("name", "")
                        tool_input = tool.get("input", {})
                        
                        if tool_name == "bash" and use_bash and sandbox_service:
                            command = tool_input.get("command", "")
                            logger.info(f"[ScheduleAgent] Executing bash: {command[:100]}")
                            
                            # Track execution time
                            exec_start = time_module.time()
                            exec_result = await sandbox_service.exec(sandbox_session_id, command)
                            exec_latency = int((time_module.time() - exec_start) * 1000)
                            
                            if exec_result.get("success"):
                                output = exec_result.get("output", "")
                                result["tool_calls"].append({
                                    "command": command,
                                    "output": output[:500],
                                    "success": True,
                                })
                                # Log bash execution
                                await log_bash_execution(
                                    command=command,
                                    agent_id=request.agent_id,
                                    session_id=request.session_id,
                                    sandbox_session_id=sandbox_session_id,
                                    success=True,
                                    output=output,
                                    latency_ms=exec_latency,
                                )
                            else:
                                output = exec_result.get("error", "")
                                result["tool_calls"].append({
                                    "command": command,
                                    "output": output[:500],
                                    "success": False,
                                })
                                # Log failed bash execution
                                await log_bash_execution(
                                    command=command,
                                    agent_id=request.agent_id,
                                    session_id=request.session_id,
                                    sandbox_session_id=sandbox_session_id,
                                    success=False,
                                    error_message=output,
                                    latency_ms=exec_latency,
                                )
                        else:
                            output = f"Unknown tool: {tool_name}"
                        
                        tool_results.append({
                            "type": "tool_result",
                            "tool_use_id": tool.get("id", ""),
                            "content": output,
                        })
                    
                    messages.append({"role": "assistant", "content": response_content})
                    messages.append({"role": "user", "content": tool_results})
                    
                    if stop_reason == "end_turn":
                        break
                        
                except Exception as e:
                    logger.error(f"[ScheduleAgent] Claude error: {e}")
                    result["status"] = "failed"
                    result["error"] = str(e)
                    break
            
            # ========== 7. å›å†™æ•°æ®åˆ°æ•°æ®åº“ ==========
            if use_bash and sandbox_service and sandbox_session_id and not sandbox_readonly:
                if sandbox_data and sandbox_data.node_path_map and node_service:
                    for node_id, info in sandbox_data.node_path_map.items():
                        if info.get("readonly"):
                            continue
                        
                        node_type = info.get("node_type", "")
                        sandbox_path = info.get("path", "")
                        json_path_config = info.get("json_path", "")
                        
                        if node_type not in ("json", "markdown"):
                            continue
                        
                        try:
                            parse_json = (node_type == "json")
                            read_result = await sandbox_service.read_file(
                                sandbox_session_id, sandbox_path, parse_json=parse_json
                            )
                            
                            if not read_result.get("success"):
                                continue
                            
                            sandbox_content = read_result.get("content")
                            node = node_service.get_by_id_unsafe(node_id)
                            if not node:
                                continue
                            
                            if node_type == "json":
                                if json_path_config:
                                    updated_data = merge_data_by_path(
                                        node.preview_json or {}, json_path_config, sandbox_content
                                    )
                                else:
                                    updated_data = sandbox_content
                                
                                node_service.update_node(
                                    node_id=node_id,
                                    project_id=node.project_id,
                                    preview_json=updated_data,
                                )
                            elif node_type == "markdown":
                                await node_service.update_markdown_content(
                                    node_id=node_id,
                                    project_id=node.project_id,
                                    content=sandbox_content,
                                )
                            
                            result["updated_nodes"].append({
                                "nodeId": node_id,
                                "nodeName": node.name,
                            })
                            logger.info(f"[ScheduleAgent] Wrote back data to node: {node_id}")
                            
                        except Exception as e:
                            logger.warning(f"[ScheduleAgent] Failed to write back: {e}")
                
                # åœæ­¢æ²™ç›’
                await sandbox_service.stop(sandbox_session_id)
                logger.info(f"[ScheduleAgent] Sandbox stopped")
            elif sandbox_session_id and sandbox_service:
                await sandbox_service.stop(sandbox_session_id)
            
            # ========== 8. è¿”å›ç»“æœ ==========
            result["output_summary"] = "\n".join(all_text_outputs)[:2000]
            logger.info(f"[ScheduleAgent] Execution completed: {result['status']}")
            return result
            
        except Exception as e:
            logger.error(f"[ScheduleAgent] Execution failed: {e}")
            return {"status": "failed", "error": str(e)}

    async def stream_events(
        self,
        request: AgentRequest,
        current_user,
        node_service,  # ContentNodeServiceï¼Œç”¨äºè·å– content_nodes æ•°æ®
        tool_service,
        sandbox_service,
        chat_service: Optional[ChatService] = None,
        s3_service=None,  # S3Serviceï¼Œç”¨äºä¸‹è½½æ–‡ä»¶
        agent_config_service: Optional[AgentConfigService] = None,  # æ–°ç‰ˆ agent é…ç½®æœåŠ¡
        search_service=None,  # SearchServiceï¼Œç”¨äº search tool æ‰§è¡Œ
        max_iterations: int = 15,
    ) -> AsyncGenerator[dict, None]:
        """
        å¤„ç† Agent è¯·æ±‚çš„ä¸»å…¥å£
        
        æ”¯æŒçš„å·¥å…·ç±»å‹ï¼š
        1. bash (sandbox) â€” é€šè¿‡ agent_bash é…ç½®
        2. search â€” é€šè¿‡ agent_tool å…³è”çš„ search ç±»å‹å·¥å…·
        """
        
        # ========== 1. è§£æé…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨æ–°ç‰ˆ agent_access ==========
        bash_tools: list[dict] = []  # [{node_id, json_path, readonly}, ...]
        
        logger.info(f"[Agent DEBUG] agent_id={request.agent_id}, active_tool_ids={request.active_tool_ids}, user_id={current_user.user_id if current_user else None}")
        
        # æ–°ç‰ˆï¼šå¦‚æœæœ‰ agent_idï¼Œä» agent_bash è¡¨è¯»å–é…ç½®
        if request.agent_id and current_user and agent_config_service:
            try:
                agent = agent_config_service.get_agent(request.agent_id)
                logger.info(f"[Agent DEBUG] Got agent: {agent.id if agent else None}, project_id={agent.project_id if agent else None}")
                
                # éªŒè¯æƒé™ï¼šé€šè¿‡ project_id æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æƒé™è®¿é—®æ­¤ Agent
                # æ³¨æ„ï¼šAgent æ¨¡å‹æ²¡æœ‰ user_id å­—æ®µï¼Œéœ€è¦é€šè¿‡ project è¡¨éªŒè¯
                has_access = False
                if agent:
                    has_access = agent_config_service.verify_access(request.agent_id, current_user.user_id)
                    logger.info(f"[Agent DEBUG] Access check: has_access={has_access}")
                
                if agent and has_access:
                    logger.info(f"[Agent] Found agent config: id={agent.id}, bash_accesses={len(agent.bash_accesses)}")
                    # æ”¶é›†æ‰€æœ‰ Bash è®¿é—®æƒé™ï¼ˆæ–°ç‰ˆæ¶æ„ä¸‹æ‰€æœ‰ bash_accesses éƒ½æ˜¯ç»ˆç«¯è®¿é—®ï¼‰
                    for bash in agent.bash_accesses:
                        bash_tools.append({
                            "node_id": bash.node_id,
                            "json_path": (bash.json_path or "").strip(),
                            "readonly": bash.readonly,
                        })
                        logger.info(f"[Agent] Found bash access from agent_bash: node_id={bash.node_id}")
                    logger.info(f"[Agent] Total bash accesses collected: {len(bash_tools)}")
                else:
                    logger.warning(f"[Agent] Agent not found or unauthorized: agent_id={request.agent_id}, has_access={has_access}")
            except Exception as e:
                logger.warning(f"[Agent] Failed to get agent config: {e}", exc_info=True)
        
        # NOTE: Legacy fallback to tool table for shell_access has been removed.
        # Shell/bash access is now managed exclusively via agent_bash table.
        # See architecture: agents â†’ agent_bash (data access) + agent_tool (tool bindings)
        
        # ========== 1b. æ”¶é›† Search Toolsï¼ˆä» agent_tool ç»‘å®šï¼‰ ==========
        search_tools_map: dict[str, SearchToolConfig] = {}  # {claude_tool_name: SearchToolConfig}
        
        if request.agent_id and current_user and agent_config_service and tool_service and search_service:
            try:
                agent_for_tools = agent_config_service.get_agent(request.agent_id)
                
                if agent_for_tools and agent_for_tools.tools:
                    used_names: set[str] = set()
                    for agent_tool_binding in agent_for_tools.tools:
                        if not agent_tool_binding.enabled:
                            continue
                        
                        # ä» tool è¡¨åŠ è½½å®Œæ•´ä¿¡æ¯
                        tool_info = tool_service.get_by_id(agent_tool_binding.tool_id)
                        if not tool_info or tool_info.type != "search":
                            continue
                        
                        # è·å–èŠ‚ç‚¹ä¿¡æ¯ä»¥ç¡®å®šæœç´¢ç±»å‹
                        try:
                            node = node_service.get_by_id_unsafe(tool_info.node_id)
                            if not node:
                                continue
                        except Exception:
                            continue
                        
                        # ç”Ÿæˆ Claude å…¼å®¹çš„å·¥å…·åç§°ï¼ˆé¿å…å†²çªï¼‰
                        base_name = _sanitize_tool_name(tool_info.name)
                        claude_name = f"search_{base_name}"
                        if claude_name in used_names:
                            claude_name = f"search_{base_name}_{tool_info.id[:8]}"
                        used_names.add(claude_name)
                        
                        search_tools_map[claude_name] = SearchToolConfig(
                            tool_id=tool_info.id,
                            node_id=tool_info.node_id,
                            json_path=tool_info.json_path or "",
                            project_id=tool_info.project_id or node.project_id,
                            node_type=node.type or "json",
                            name=tool_info.name,
                            description=tool_info.description or f"Search in {tool_info.name}",
                            claude_tool_name=claude_name,
                        )
                        logger.info(f"[Agent] Loaded search tool: {claude_name} (tool_id={tool_info.id}, node_type={node.type})")
                    
                    if search_tools_map:
                        logger.info(f"[Agent] Total search tools: {len(search_tools_map)}")
            except Exception as e:
                logger.warning(f"[Agent] Failed to load search tools: {e}", exc_info=True)
        
        # ========== 2. Chat persistence (best-effort) ==========
        persisted_session_id: str | None = None
        created_session = False
        should_persist = current_user is not None and chat_service is not None
        
        logger.info(f"[Chat Persist] should_persist={should_persist}, current_user={current_user is not None}, chat_service={chat_service is not None}")
        
        if should_persist:
            try:
                persisted_session_id, created_session = chat_service.ensure_session(
                    user_id=current_user.user_id,
                    session_id=request.session_id,
                    agent_id=request.agent_id,
                    mode="agent",
                )
                logger.info(f"[Chat Persist] Session ready: id={persisted_session_id}, created={created_session}")
                # å¦‚æœæ˜¯æ–°å»ºçš„ sessionï¼Œå…ˆè®¾ç½® titleï¼Œå†é€šçŸ¥å‰ç«¯
                if created_session and persisted_session_id:
                    try:
                        chat_service.maybe_set_title_on_first_message(
                            user_id=current_user.user_id,
                            session_id=persisted_session_id,
                            first_message=request.prompt,
                        )
                        logger.info(f"[Chat Persist] Session title set for {persisted_session_id}")
                    except Exception as e:
                        logger.warning(f"[Chat Persist] Failed to set session title: {e}")
                    yield {"type": "session", "sessionId": persisted_session_id}
            except Exception as e:
                logger.error(f"[Chat Persist] Failed to ensure session: {e}")
                persisted_session_id = None
                should_persist = False

        # åŠ è½½å†å²æ¶ˆæ¯
        messages: list[dict[str, Any]] = []
        if should_persist and persisted_session_id:
            try:
                history = chat_service.load_history_for_llm(
                    user_id=current_user.user_id, session_id=persisted_session_id, limit=60
                )
                messages.extend(history)
            except Exception:
                if request.chatHistory:
                    for item in request.chatHistory:
                        messages.append({"role": item.role, "content": item.content})
        else:
            if request.chatHistory:
                for item in request.chatHistory:
                    messages.append({"role": item.role, "content": item.content})

        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        if should_persist and persisted_session_id:
            try:
                chat_service.add_user_message(session_id=persisted_session_id, content=request.prompt)
                logger.info(f"[Chat Persist] User message saved to session {persisted_session_id}")
            except Exception as e:
                logger.error(f"[Chat Persist] Failed to save user message: {e}")

        # ========== 3. å¦‚æœæœ‰ bash toolsï¼Œå‡†å¤‡æ²™ç›’æ•°æ®ã€å¯åŠ¨æ²™ç›’ ==========
        use_bash = len(bash_tools) > 0
        sandbox_data: SandboxData | None = None
        sandbox_session_id = None
        # å¦‚æœä»»æ„ä¸€ä¸ª access ä¸æ˜¯ readonlyï¼Œåˆ™æ•´ä¸ªæ²™ç›’ä¸æ˜¯ readonly
        sandbox_readonly = all(tool["readonly"] for tool in bash_tools) if bash_tools else True
        
        if use_bash and node_service and current_user:
            # æ”¶é›†æ‰€æœ‰ bash access çš„æ–‡ä»¶
            all_files: list[SandboxFile] = []
            primary_node_type = "folder"  # é»˜è®¤ç±»å‹
            primary_node_id = ""
            primary_node_name = ""
            # è¿½è¸ªæ¯ä¸ª node å¯¹åº”çš„æ²™ç›’è·¯å¾„å’Œç±»å‹ï¼Œç”¨äºå›å†™
            node_path_map: dict = {}  # {node_id: {path, node_type, json_path}}
            
            for i, tool in enumerate(bash_tools):
                try:
                    data = await prepare_sandbox_data(
                        node_service=node_service,
                        node_id=tool["node_id"],
                        json_path=tool["json_path"],
                        user_id=current_user.user_id,
                    )
                    logger.info(f"[Agent] Prepared sandbox data for access {i+1}/{len(bash_tools)}: "
                               f"node_id={tool['node_id']}, type={data.node_type}, files={len(data.files)}")
                    all_files.extend(data.files)
                    
                    # Log context access (data egress tracking)
                    await log_context_access(
                        node_id=tool["node_id"],
                        node_type=data.node_type,
                        node_name=data.root_node_name,
                        user_id=current_user.user_id if current_user else None,
                        agent_id=request.agent_id,
                        session_id=request.session_id,
                    )
                    
                    # è®°å½•è·¯å¾„æ˜ å°„ï¼ˆç”¨äºå›å†™å’Œæ˜¾ç¤ºï¼‰
                    if data.files:
                        # å¯¹äº JSON å’Œå•æ–‡ä»¶ï¼Œè®°å½•ä¸»æ–‡ä»¶è·¯å¾„
                        main_path = data.files[0].path
                        node_path_map[tool["node_id"]] = {
                            "path": main_path,
                            "node_type": data.node_type,
                            "json_path": tool["json_path"],
                            "readonly": tool["readonly"],
                        }
                    else:
                        # ç©ºæ–‡ä»¶å¤¹ä¹Ÿè®°å½•ï¼Œä½¿ç”¨æ–‡ä»¶å¤¹åä½œä¸ºè·¯å¾„
                        node_path_map[tool["node_id"]] = {
                            "path": f"/workspace/{data.root_node_name}" if data.root_node_name else "/workspace/(empty folder)",
                            "node_type": data.node_type,
                            "json_path": tool["json_path"],
                            "readonly": tool["readonly"],
                            "is_empty": True,
                        }
                    
                    # ç¬¬ä¸€ä¸ª access å†³å®šä¸»ç±»å‹
                    if i == 0:
                        primary_node_type = data.node_type
                        primary_node_id = data.root_node_id
                        primary_node_name = data.root_node_name
                except Exception as e:
                    logger.warning(f"[Agent] Failed to prepare sandbox data for node {tool['node_id']}: {e}")
            
            sandbox_data = SandboxData(
                files=all_files,
                node_type=primary_node_type if len(bash_tools) == 1 else "multi",  # å¤šä¸ªæ—¶æ ‡è®°ä¸º multi
                root_node_id=primary_node_id,
                root_node_name=primary_node_name,
                node_path_map=node_path_map,
            )
            logger.info(f"[Agent] Total sandbox files: {len(all_files)} from {len(bash_tools)} accesses, path_map={list(node_path_map.keys())}")

        if use_bash and sandbox_service:
            sandbox_session_id = f"agent-{int(time.time() * 1000)}"
            
            # æ ¹æ®èŠ‚ç‚¹ç±»å‹é€‰æ‹©å¯åŠ¨æ–¹å¼
            if sandbox_data and sandbox_data.node_type == "json" and len(bash_tools) == 1:
                # å•ä¸ª JSON èŠ‚ç‚¹ï¼šä½¿ç”¨ç°æœ‰çš„ data å‚æ•°
                json_content = {}
                if sandbox_data.files:
                    try:
                        json_content = json.loads(sandbox_data.files[0].content or "{}")
                    except:
                        json_content = {}
                start_result = await sandbox_service.start(
                    session_id=sandbox_session_id,
                    data=json_content,
                    readonly=sandbox_readonly,
                )
            else:
                # Folder/File/å¤šä¸ªèŠ‚ç‚¹ï¼šä½¿ç”¨ files å‚æ•°
                start_result = await sandbox_service.start_with_files(
                    session_id=sandbox_session_id,
                    files=sandbox_data.files if sandbox_data else [],
                    readonly=sandbox_readonly,
                    s3_service=s3_service,
                )
            
            if not start_result.get("success"):
                err_msg = start_result.get("error", "Failed to start sandbox")
                yield {"type": "error", "message": err_msg}
                if should_persist and persisted_session_id:
                    try:
                        chat_service.add_assistant_message(
                            session_id=persisted_session_id,
                            content=err_msg,
                            parts=[{"type": "text", "content": err_msg}],
                        )
                    except Exception:
                        pass
                return
            
            yield {"type": "status", "message": "Sandbox ready"}

        # ========== 4. æ„å»º Claude è¯·æ±‚ ==========
        tools: list[dict[str, Any]] = [_get_bash_tool()] if use_bash else []
        
        # æ³¨å†Œ Search Tools åˆ° Claude
        for claude_name, stc in search_tools_map.items():
            tools.append({
                "name": claude_name,
                "description": stc.description,
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "The search query text",
                        },
                        "top_k": {
                            "type": "integer",
                            "description": "Number of results to return (default 5, max 20)",
                            "default": 5,
                        },
                    },
                    "required": ["query"],
                },
            })
        
        use_search = len(search_tools_map) > 0
        
        # æ ¹æ®èŠ‚ç‚¹ç±»å‹æ„å»ºç³»ç»Ÿæç¤º
        if use_bash and sandbox_data:
            node_type = sandbox_data.node_type
            if node_type == "json":
                system_prompt = "You are an AI agent specialized in JSON data processing. Use the bash tool to view and manipulate data in /workspace/data.json."
            elif node_type == "folder":
                system_prompt = "You are an AI agent with access to a file system. Use the bash tool to explore and manipulate files in /workspace/."
            elif node_type == "multi":
                system_prompt = "You are an AI agent with access to multiple files and folders. Use the bash tool to explore and manipulate files in /workspace/."
            else:
                system_prompt = f"You are an AI agent with access to a {node_type} file. Use the bash tool to analyze the file in /workspace/."
        else:
            system_prompt = "You are an AI agent, a helpful assistant."

        # å¦‚æœæœ‰ search toolsï¼Œåœ¨ç³»ç»Ÿæç¤ºä¸­è¡¥å……è¯´æ˜
        if use_search:
            search_tool_descriptions = []
            for claude_name, stc in search_tools_map.items():
                search_tool_descriptions.append(
                    f"  - {claude_name}: {stc.description} (data source: {stc.name})"
                )
            search_prompt_suffix = (
                "\n\n[Available Search Tools]\n"
                "You have access to the following search tools for semantic retrieval:\n"
                + "\n".join(search_tool_descriptions) +
                "\nUse these tools when the user asks questions about the data. "
                "Pass a natural language query to search for relevant information."
            )
            system_prompt += search_prompt_suffix

        # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼šå¦‚æœä½¿ç”¨ bashï¼Œæ·»åŠ æ•°æ®ä¸Šä¸‹æ–‡
        user_content = request.prompt
        if use_bash and sandbox_data:
            node_type = sandbox_data.node_type
            node_path_map = sandbox_data.node_path_map or {}
            
            # ç”Ÿæˆè¯¦ç»†çš„æƒé™æ¸…å•
            def build_access_list() -> str:
                lines = []
                for tool in bash_tools:
                    path_info = node_path_map.get(tool["node_id"], {})
                    path = path_info.get("path", "/workspace/(unknown)")
                    mode = "ğŸ‘ï¸ View Only" if tool["readonly"] else "âœï¸ Editable"
                    is_empty = path_info.get("is_empty", False)
                    suffix = " ğŸ“ (empty folder)" if is_empty else ""
                    lines.append(f"  - {path} ({mode}){suffix}")
                return "\n".join(lines) if lines else "  - /workspace/ (unknown)"
            
            if node_type == "json" and len(bash_tools) == 1:
                json_path = bash_tools[0]["json_path"] or "/"
                mode_str = "âš ï¸ åªè¯»æ¨¡å¼ - ä¿®æ”¹ä¸ä¼šè¢«ä¿å­˜" if sandbox_readonly else "âœï¸ å¯è¯»å†™æ¨¡å¼"
                context_prefix = (
                    f"[æ•°æ®ä¸Šä¸‹æ–‡]\n"
                    f"èŠ‚ç‚¹ç±»å‹: JSON\n"
                    f"æ•°æ®æ–‡ä»¶: /workspace/data.json\n"
                    f"JSON è·¯å¾„: {json_path}\n"
                    f"æƒé™: {mode_str}\n"
                    f"[ç”¨æˆ·æ¶ˆæ¯]\n"
                )
            elif node_type == "folder" and len(bash_tools) == 1:
                mode_str = "âš ï¸ åªè¯»æ¨¡å¼ - ä¿®æ”¹ä¸ä¼šè¢«ä¿å­˜" if sandbox_readonly else "âœï¸ å¯è¯»å†™æ¨¡å¼"
                context_prefix = (
                    f"[æ•°æ®ä¸Šä¸‹æ–‡]\n"
                    f"èŠ‚ç‚¹ç±»å‹: æ–‡ä»¶å¤¹\n"
                    f"æ–‡ä»¶å¤¹å: {sandbox_data.root_node_name}\n"
                    f"å·¥ä½œç›®å½•: /workspace/\n"
                    f"æ–‡ä»¶æ•°é‡: {len(sandbox_data.files)}\n"
                    f"æƒé™: {mode_str}\n"
                    f"[ç”¨æˆ·æ¶ˆæ¯]\n"
                )
            elif node_type == "multi" or len(bash_tools) > 1:
                # å¤šä¸ª access - æ˜¾ç¤ºè¯¦ç»†æƒé™æ¸…å•
                access_list = build_access_list()
                context_prefix = (
                    f"[æ•°æ®ä¸Šä¸‹æ–‡]\n"
                    f"å·¥ä½œç›®å½•: /workspace/\n"
                    f"æ€»æ–‡ä»¶æ•°: {len(sandbox_data.files)}\n\n"
                    f"ğŸ“‚ å¯è®¿é—®çš„èµ„æº:\n"
                    f"{access_list}\n\n"
                    f"âš ï¸ é‡è¦æç¤º:\n"
                    f"  - åªæœ‰ä¸Šè¿°è·¯å¾„çš„å†…å®¹ä¼šè¢«ä¿å­˜\n"
                    f"  - /workspace/ æ ¹ç›®å½•ä¸èƒ½åˆ›å»ºæ–°æ–‡ä»¶\n"
                    f"  - View Only è·¯å¾„çš„ä¿®æ”¹ä¸ä¼šè¢«æŒä¹…åŒ–\n"
                    f"[ç”¨æˆ·æ¶ˆæ¯]\n"
                )
            else:
                # file/pdf/image ç­‰å•ä¸ªæ–‡ä»¶
                file_name = sandbox_data.files[0].path.split("/")[-1] if sandbox_data.files else "file"
                mode_str = "âš ï¸ åªè¯»æ¨¡å¼ - ä¿®æ”¹ä¸ä¼šè¢«ä¿å­˜" if sandbox_readonly else "âœï¸ å¯è¯»å†™æ¨¡å¼"
                context_prefix = (
                    f"[æ•°æ®ä¸Šä¸‹æ–‡]\n"
                    f"èŠ‚ç‚¹ç±»å‹: {node_type}\n"
                    f"æ–‡ä»¶å: {file_name}\n"
                    f"æ–‡ä»¶è·¯å¾„: /workspace/{file_name}\n"
                    f"æƒé™: {mode_str}\n"
                    f"[ç”¨æˆ·æ¶ˆæ¯]\n"
                )
            user_content = context_prefix + request.prompt
        
        messages.append({"role": "user", "content": user_content})

        # ========== 5. è°ƒç”¨ Claude (æµå¼)ï¼Œå¤„ç†å·¥å…·è°ƒç”¨ ==========
        persisted_parts: list[dict[str, Any]] = []
        tool_index = 0
        iterations = 0

        while iterations < max_iterations:
            iterations += 1
            
            logger.info(f"[CLAUDE REQUEST] Iteration {iterations} (streaming)")
            logger.info(f"[CLAUDE DEBUG] system_prompt = {system_prompt}")
            logger.info(f"[CLAUDE DEBUG] tools = {json.dumps(tools, ensure_ascii=False)}")
            logger.info(f"[CLAUDE DEBUG] messages = {json.dumps(messages, ensure_ascii=False, default=str)[:2000]}")
            
            try:
                # ===== æµå¼è°ƒç”¨ Claude =====
                current_text_content = ""
                tool_uses: list[dict[str, Any]] = []
                current_tool: dict[str, Any] | None = None
                current_tool_input_json = ""
                stop_reason = None
                response_content: list[Any] = []
                
                # æ„å»º API è°ƒç”¨å‚æ•°ï¼ˆtools ä¸ºç©ºæ—¶ä¸ä¼ é€’ï¼Œé¿å…ä»£ç†ç½‘å…³å…¼å®¹é—®é¢˜ï¼‰
                stream_kwargs: dict[str, Any] = {
                    "model": settings.ANTHROPIC_MODEL,
                    "max_tokens": 4096,
                    "system": system_prompt,
                    "messages": messages,
                }
                if tools:
                    stream_kwargs["tools"] = tools
                
                async with self._anthropic.messages.stream(**stream_kwargs) as stream:
                    async for event in stream:
                        event_type = getattr(event, "type", None)
                        
                        if event_type == "content_block_start":
                            block = getattr(event, "content_block", None)
                            block_type = getattr(block, "type", None) if block else None
                            
                            if block_type == "text":
                                current_text_content = ""
                            elif block_type == "tool_use":
                                current_tool = {
                                    "id": getattr(block, "id", ""),
                                    "name": getattr(block, "name", ""),
                                    "input": {},
                                }
                                current_tool_input_json = ""
                        
                        elif event_type == "content_block_delta":
                            delta = getattr(event, "delta", None)
                            delta_type = getattr(delta, "type", None) if delta else None
                            
                            if delta_type == "text_delta":
                                text = getattr(delta, "text", "")
                                if text:
                                    current_text_content += text
                                    # å®æ—¶ yield æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µï¼
                                    yield {"type": "text_delta", "content": text}
                            
                            elif delta_type == "input_json_delta":
                                partial_json = getattr(delta, "partial_json", "")
                                if partial_json:
                                    current_tool_input_json += partial_json
                        
                        elif event_type == "content_block_stop":
                            if current_text_content:
                                # æ–‡æœ¬å—ç»“æŸï¼Œä¿å­˜å®Œæ•´æ–‡æœ¬
                                persisted_parts.append({"type": "text", "content": current_text_content})
                                response_content.append({"type": "text", "text": current_text_content})
                                current_text_content = ""
                            
                            if current_tool:
                                # å·¥å…·å—ç»“æŸï¼Œè§£æ JSON è¾“å…¥
                                try:
                                    current_tool["input"] = json.loads(current_tool_input_json) if current_tool_input_json else {}
                                except json.JSONDecodeError:
                                    current_tool["input"] = {"raw": current_tool_input_json}
                                
                                tool_uses.append(current_tool)
                                response_content.append({
                                    "type": "tool_use",
                                    "id": current_tool["id"],
                                    "name": current_tool["name"],
                                    "input": current_tool["input"],
                                })
                                current_tool = None
                                current_tool_input_json = ""
                        
                        elif event_type == "message_delta":
                            delta = getattr(event, "delta", None)
                            if delta:
                                stop_reason = getattr(delta, "stop_reason", None)
                
                logger.info(f"[CLAUDE RESPONSE] Iteration {iterations}: stop_reason={stop_reason}, tool_uses={len(tool_uses)}")
                
            except Exception as e:
                msg = str(e)
                logger.error(f"[CLAUDE ERROR] {msg}")
                yield {"type": "error", "message": msg}
                if should_persist and persisted_session_id:
                    try:
                        chat_service.add_assistant_message(
                            session_id=persisted_session_id,
                            content=msg,
                            parts=[{"type": "text", "content": msg}],
                        )
                    except Exception:
                        pass
                if sandbox_session_id and sandbox_service:
                    try:
                        await sandbox_service.stop(sandbox_session_id)
                    except Exception:
                        pass
                return

            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¾ªç¯
            if not tool_uses:
                break

            # æ‰§è¡Œå·¥å…·
            tool_results = []
            for tool in tool_uses:
                current_tool_index = tool_index
                tool_index += 1
                tool_name = tool.get("name", "")
                tool_input = tool.get("input", {})
                
                yield {
                    "type": "tool_start",
                    "toolId": current_tool_index,
                    "toolName": tool_name,
                    "toolInput": tool_input.get("command") if tool_name == "bash" else json.dumps(tool_input),
                }
                
                persisted_parts.append({
                    "type": "tool",
                    "toolId": str(current_tool_index),
                    "toolName": tool_name or "tool",
                    "toolInput": tool_input.get("command") if tool_name == "bash" else json.dumps(tool_input),
                    "toolStatus": "running",
                })

                success = True
                output = ""
                
                if tool_name == "bash" and use_bash and sandbox_service:
                    command = tool_input.get("command", "")
                    
                    # Track execution time
                    exec_start = time_module.time()
                    exec_result = await sandbox_service.exec(sandbox_session_id, command)
                    exec_latency = int((time_module.time() - exec_start) * 1000)
                    
                    if exec_result.get("success"):
                        output = exec_result.get("output", "")
                        # Log bash execution
                        await log_bash_execution(
                            command=command,
                            user_id=current_user.user_id if current_user else None,
                            agent_id=request.agent_id,
                            session_id=persisted_session_id,  # Use chat session id
                            sandbox_session_id=sandbox_session_id,
                            success=True,
                            output=output,
                            latency_ms=exec_latency,
                        )
                    else:
                        success = False
                        output = exec_result.get("error", "")
                        # Log failed bash execution
                        await log_bash_execution(
                            command=command,
                            user_id=current_user.user_id if current_user else None,
                            agent_id=request.agent_id,
                            session_id=persisted_session_id,
                            sandbox_session_id=sandbox_session_id,
                            success=False,
                            error_message=output,
                            latency_ms=exec_latency,
                        )
                elif tool_name in search_tools_map and search_service:
                    # ===== Search Tool æ‰§è¡Œ =====
                    stc = search_tools_map[tool_name]
                    query = tool_input.get("query", "")
                    top_k = tool_input.get("top_k", 5)
                    
                    exec_start = time_module.time()
                    try:
                        if stc.node_type == "folder":
                            results = await search_service.search_folder(
                                project_id=stc.project_id,
                                folder_node_id=stc.node_id,
                                query=query,
                                top_k=top_k,
                            )
                        else:
                            results = await search_service.search_scope(
                                project_id=stc.project_id,
                                node_id=stc.node_id,
                                tool_json_path=stc.json_path,
                                query=query,
                                top_k=top_k,
                            )
                        exec_latency = int((time_module.time() - exec_start) * 1000)
                        output = json.dumps(results, ensure_ascii=False, indent=2)
                        success = True
                        logger.info(
                            f"[Agent] Search tool executed: {tool_name}, query='{query}', "
                            f"results={len(results)}, latency={exec_latency}ms"
                        )
                    except Exception as e:
                        exec_latency = int((time_module.time() - exec_start) * 1000)
                        output = f"Search error: {str(e)}"
                        success = False
                        logger.error(f"[Agent] Search tool failed: {tool_name}, error={e}, latency={exec_latency}ms")
                else:
                    output = f"Unknown tool: {tool_name}"
                    success = False

                yield {
                    "type": "tool_end",
                    "toolId": current_tool_index,
                    "toolName": tool_name,
                    "output": output[:500],
                    "success": success,
                }
                
                # æ›´æ–°æŒä¹…åŒ–çŠ¶æ€
                for i in range(len(persisted_parts) - 1, -1, -1):
                    p = persisted_parts[i]
                    if p.get("type") == "tool" and p.get("toolId") == str(current_tool_index):
                        p["toolStatus"] = "completed" if success else "error"
                        p["toolOutput"] = output[:500]
                        break

                tool_results.append({
                    "type": "tool_result",
                    "tool_use_id": tool.get("id", ""),
                    "content": output,
                    "is_error": not success,
                })

            messages.append({"role": "assistant", "content": response_content})
            messages.append({"role": "user", "content": tool_results})
            
            if stop_reason == "end_turn":
                break

        # ========== 6. ä¿å­˜ç»“æœã€æ¸…ç†æ²™ç›’ ==========
        logger.info(f"[Chat Persist] Attempting to save assistant message: should_persist={should_persist}, session_id={persisted_session_id}, parts_count={len(persisted_parts)}")
        if should_persist and persisted_session_id:
            try:
                for p in persisted_parts:
                    if p.get("type") == "tool" and p.get("toolStatus") == "running":
                        p["toolStatus"] = "completed"
                final_content = "\n\n".join(
                    [p.get("content", "") for p in persisted_parts if p.get("type") == "text"]
                ).strip()
                logger.info(f"[Chat Persist] Saving assistant message: content_length={len(final_content)}, parts={persisted_parts}")
                chat_service.add_assistant_message(
                    session_id=persisted_session_id, content=final_content, parts=persisted_parts
                )
                logger.info(f"[Chat Persist] Assistant message saved successfully!")
            except Exception as e:
                logger.error(f"[Chat Persist] Failed to save assistant message: {e}")
        else:
            logger.warning(f"[Chat Persist] Skipping assistant message save: should_persist={should_persist}, session_id={persisted_session_id}")

        if use_bash and sandbox_service and sandbox_session_id:
            # å›å†™ä¿®æ”¹çš„æ•°æ®åˆ°æ•°æ®åº“
            updated_nodes = []
            
            if sandbox_data and sandbox_data.node_path_map and node_service and current_user:
                # éå†æ‰€æœ‰éåªè¯»çš„ accessï¼Œå›å†™æ•°æ®
                for node_id, info in sandbox_data.node_path_map.items():
                    if info.get("readonly"):
                        continue  # è·³è¿‡åªè¯»çš„
                    
                    node_type = info.get("node_type", "")
                    sandbox_path = info.get("path", "")
                    json_path = info.get("json_path", "")
                    
                    # æ”¯æŒå›å†™çš„èŠ‚ç‚¹ç±»å‹ï¼šjson, markdown
                    if node_type not in ("json", "markdown"):
                        logger.info(f"[Agent] Skipping write-back for unsupported node type: {node_id} (type={node_type})")
                        continue
                    
                    try:
                        # ä»æ²™ç›’è¯»å–æ–‡ä»¶
                        # JSON æ–‡ä»¶éœ€è¦è§£æï¼Œmarkdown æ–‡ä»¶ä¿æŒåŸæ ·
                        parse_json = (node_type == "json")
                        read_result = await sandbox_service.read_file(
                            sandbox_session_id, 
                            sandbox_path, 
                            parse_json=parse_json
                        )
                        
                        if not read_result.get("success"):
                            logger.warning(f"[Agent] Failed to read file from sandbox: {sandbox_path}")
                            continue
                        
                        sandbox_content = read_result.get("content")
                        
                        # è·å–åŸèŠ‚ç‚¹æ•°æ®ï¼ˆå†…éƒ¨æ“ä½œï¼Œæƒé™å·²é€šè¿‡ access points éªŒè¯ï¼‰
                        node = node_service.get_by_id_unsafe(node_id)
                        if not node:
                            continue
                        
                        # æ ¹æ®èŠ‚ç‚¹ç±»å‹é€‰æ‹©ä¸åŒçš„æ›´æ–°æ–¹å¼
                        if node_type == "json":
                            # JSON ç±»å‹ï¼šåˆå¹¶æ•°æ®ï¼ˆå¦‚æœæœ‰ json_pathï¼‰
                            if json_path:
                                updated_data = merge_data_by_path(
                                    node.preview_json or {}, json_path, sandbox_content
                                )
                            else:
                                updated_data = sandbox_content
                            
                            # å†™å›æ•°æ®åº“ï¼ˆpreview_json å­—æ®µï¼‰
                            node_service.update_node(
                                node_id=node_id,
                                project_id=node.project_id,
                                preview_json=updated_data,
                            )
                        elif node_type == "markdown":
                            # markdown ç±»å‹ï¼šä¸Šä¼ åˆ° S3
                            await node_service.update_markdown_content(
                                node_id=node_id,
                                project_id=node.project_id,
                                content=sandbox_content,
                            )
                        
                        updated_nodes.append({
                            "nodeId": node_id,
                            "nodeName": node.name,
                            "modifiedPath": json_path,
                        })
                        logger.info(f"[Agent] Successfully wrote back data to node: {node_id}")
                        
                    except Exception as e:
                        logger.warning(f"[Agent] Failed to write back data for node {node_id}: {e}")
            
            # è¿”å›ç»“æœ
            if updated_nodes:
                yield {
                    "type": "result",
                    "success": True,
                    "updatedNodes": updated_nodes,
                }
            else:
                yield {"type": "result", "success": True}
            
            await sandbox_service.stop(sandbox_session_id)
        else:
            yield {"type": "result", "success": True}


# ========== å·¥å…·å‡½æ•° ==========

async def prepare_sandbox_data(
    node_service,
    node_id: str,
    json_path: str | None,
    user_id: str,
) -> SandboxData:
    """
    ç»Ÿä¸€çš„æ²™ç›’æ•°æ®å‡†å¤‡å‡½æ•°
    
    æ ¹æ®èŠ‚ç‚¹ç±»å‹è¿”å›ä¸åŒçš„æ²™ç›’æ•°æ®ï¼š
    - json: å¯¼å‡º content ä¸º data.jsonï¼ˆå¯é€‰ json_path æå–å­æ•°æ®ï¼‰
    - folder: é€’å½’è·å–å­æ–‡ä»¶åˆ—è¡¨
    - github_repo: ä» S3 ç›®å½•ä¸‹è½½æ‰€æœ‰æ–‡ä»¶ï¼ˆå•èŠ‚ç‚¹æ¨¡å¼ï¼‰
    - file/pdf/image/etc: å•ä¸ªæ–‡ä»¶ä¿¡æ¯
    """
    # å†…éƒ¨æ“ä½œï¼Œæƒé™å·²é€šè¿‡ access points éªŒè¯
    node = node_service.get_by_id_unsafe(node_id)
    if not node:
        raise ValueError(f"Node not found: {node_id}")
    
    logger.info(f"[prepare_sandbox_data] node.id={node.id}, type={node.type}, name={node.name}")
    
    files: list[SandboxFile] = []
    node_type = node.type or "json"
    
    if node_type == "github_repo":
        # GitHub Repo èŠ‚ç‚¹ï¼ˆå•èŠ‚ç‚¹æ¨¡å¼ï¼‰ï¼šä» preview_json.files è¯»å–æ–‡ä»¶åˆ—è¡¨
        # æ¯ä¸ªæ–‡ä»¶éƒ½æœ‰ s3_keyï¼Œç”¨äºä» S3 ä¸‹è½½
        content = node.preview_json or {}
        file_list = content.get("files", [])
        repo_name = content.get("repo", node.name or "repo")
        
        logger.info(f"[prepare_sandbox_data] GitHub repo node, file_count={len(file_list)}")
        
        for file_info in file_list:
            file_path = file_info.get("path", "")
            s3_key = file_info.get("s3_key", "")
            
            if not file_path or not s3_key:
                continue
            
            # ä¿æŒ repo å†…çš„ç›®å½•ç»“æ„
            files.append(SandboxFile(
                path=f"/workspace/{repo_name}/{file_path}",
                s3_key=s3_key,
                content_type="text/plain",  # GitHub æ–‡ä»¶éƒ½æ˜¯æ–‡æœ¬
            ))
    
    elif node_type == "json":
        # JSON èŠ‚ç‚¹ï¼šå¯¼å‡º preview_json ä¸º data.json
        content = node.preview_json or {}
        if json_path:
            content = extract_data_by_path(content, json_path)
        
        files.append(SandboxFile(
            path="/workspace/data.json",
            content=json.dumps(content, ensure_ascii=False, indent=2),
            content_type="application/json",
        ))
        logger.info(f"[prepare_sandbox_data] JSON node, content size={len(str(content))}")
        
    elif node_type == "folder":
        # Folder èŠ‚ç‚¹ï¼šé€’å½’è·å–æ‰€æœ‰å­æ–‡ä»¶
        # ä½¿ç”¨ list_descendants è·å–æ‰€æœ‰å­å­™èŠ‚ç‚¹
        children = node_service.list_descendants(node.project_id, node_id)
        logger.info(f"[prepare_sandbox_data] Folder node, children count={len(children)}")
        
        # æ„å»º id -> node æ˜ å°„ï¼Œç”¨äºè®¡ç®—åç§°è·¯å¾„
        # åŒ…å«æ ¹èŠ‚ç‚¹å’Œæ‰€æœ‰å­å­™èŠ‚ç‚¹
        id_to_node: dict[str, Any] = {node.id: node}
        for child in children:
            id_to_node[child.id] = child
        
        def build_name_path(target_node) -> str:
            """æ ¹æ® parent_id é“¾æ„å»ºåç§°è·¯å¾„ï¼ˆä»æ ¹æ–‡ä»¶å¤¹ä¸‹ä¸€çº§å¼€å§‹ï¼‰"""
            path_parts = []
            current = target_node
            while current and current.id != node.id:
                path_parts.append(current.name)
                if current.parent_id and current.parent_id in id_to_node:
                    current = id_to_node[current.parent_id]
                else:
                    break
            path_parts.reverse()
            return "/".join(path_parts)
        
        for child in children:
            if child.type == "folder":
                continue  # è·³è¿‡å­æ–‡ä»¶å¤¹æœ¬èº«ï¼Œåªå¤„ç†æ–‡ä»¶
            
            # ä½¿ç”¨æ–‡ä»¶åæ„å»ºç›¸å¯¹è·¯å¾„ï¼ˆè€Œé UUIDï¼‰
            relative_path = build_name_path(child)
            if not relative_path:
                relative_path = child.name
            
            if child.type == "json":
                # JSON å­èŠ‚ç‚¹ï¼šå¯¼å‡ºä¸º .json æ–‡ä»¶
                files.append(SandboxFile(
                    path=f"/workspace/{relative_path}.json",
                    content=json.dumps(child.preview_json or {}, ensure_ascii=False, indent=2),
                    content_type="application/json",
                ))
            elif child.s3_key:
                # å…¶ä»–æ–‡ä»¶ç±»å‹ï¼šè®°å½• S3 keyï¼Œç”±æ²™ç›’æœåŠ¡ä¸‹è½½
                files.append(SandboxFile(
                    path=f"/workspace/{relative_path}",
                    s3_key=child.s3_key,
                    content_type=child.mime_type or "application/octet-stream",
                ))
    else:
        # å…¶ä»–æ–‡ä»¶ç±»å‹ï¼ˆpdf, image, file, markdown, sync ç­‰ï¼‰
        file_name = node.name or "file"
        
        if node.preview_json and isinstance(node.preview_json, (dict, list)):
            # å¦‚æœæœ‰ JSON contentï¼Œå¯¼å‡ºä¸º JSON
            files.append(SandboxFile(
                path=f"/workspace/{file_name}.json",
                content=json.dumps(node.preview_json, ensure_ascii=False, indent=2),
                content_type="application/json",
            ))
        elif node.s3_key:
            # S3 æ–‡ä»¶ï¼ˆmarkdown, image, pdf ç­‰ï¼‰
            files.append(SandboxFile(
                path=f"/workspace/{file_name}",
                s3_key=node.s3_key,
                content_type=node.mime_type or "application/octet-stream",
            ))
        else:
            logger.warning(f"[prepare_sandbox_data] Node has no content or s3_key: {node_id}")
    
    return SandboxData(
        files=files,
        node_type=node_type,
        root_node_id=node.id,
        root_node_name=node.name or "",
    )


def extract_data_by_path(data, json_path: str):
    """ä» JSON æ•°æ®ä¸­æå–æŒ‡å®šè·¯å¾„çš„èŠ‚ç‚¹"""
    if not json_path or json_path == "/" or json_path == "":
        return data

    segments = [segment for segment in json_path.split("/") if segment]
    current = data
    for segment in segments:
        if current is None:
            return None
        if isinstance(current, list):
            try:
                index = int(segment)
            except (TypeError, ValueError):
                return None
            if index < 0 or index >= len(current):
                return None
            current = current[index]
        elif isinstance(current, dict):
            current = current.get(segment)
        else:
            return None
    return current


def merge_data_by_path(original_data, json_path: str, new_node_data):
    """å°†æ–°æ•°æ®åˆå¹¶å›åŸæ•°æ®çš„æŒ‡å®šè·¯å¾„"""
    if not json_path or json_path == "/" or json_path == "":
        return new_node_data

    result = json.loads(json.dumps(original_data))
    segments = [segment for segment in json_path.split("/") if segment]

    current = result
    for segment in segments[:-1]:
        if isinstance(current, list):
            current = current[int(segment)]
        elif isinstance(current, dict):
            current = current[segment]

    last_segment = segments[-1]
    if isinstance(current, list):
        current[int(last_segment)] = new_node_data
    elif isinstance(current, dict):
        current[last_segment] = new_node_data

    return result


def _default_anthropic_client():
    from anthropic import AsyncAnthropic
    api_key = settings.ANTHROPIC_API_KEY or None
    base_url = settings.ANTHROPIC_BASE_URL or None
    return AsyncAnthropic(api_key=api_key, base_url=base_url)


def _get_attr(obj, name: str, default=None):
    if isinstance(obj, dict):
        return obj.get(name, default)
    return getattr(obj, name, default)


def _normalize_content(content: Iterable[Any]):
    normalized = []
    for block in content:
        block_type = _get_attr(block, "type")
        if block_type == "text":
            normalized.append({"type": "text", "text": _get_attr(block, "text")})
        elif block_type == "tool_use":
            normalized.append({
                "type": "tool_use",
                "id": _get_attr(block, "id"),
                "name": _get_attr(block, "name"),
                "input": _get_attr(block, "input"),
            })
    return normalized
