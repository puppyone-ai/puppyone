"""
Agent Service - ç®€åŒ–ç‰ˆ

å‰ç«¯åªéœ€è¦ä¼  active_tool_idsï¼Œåç«¯è‡ªåŠ¨å¤„ç†ä¸€åˆ‡ï¼š
1. æ ¹æ® tool_id æŸ¥åº“è·å– tool é…ç½®
2. å¦‚æœæ˜¯ bash toolï¼Œè‡ªåŠ¨æŸ¥è¡¨æ•°æ®ã€å¯åŠ¨æ²™ç›’
3. æ„å»º Claude è¯·æ±‚

æ”¯æŒçš„èŠ‚ç‚¹ç±»å‹ï¼š
- folder: é€’å½’è·å–å­æ–‡ä»¶ï¼Œåœ¨æ²™ç›’ä¸­é‡å»ºç›®å½•ç»“æ„
- json: å¯¼å‡º content ä¸º data.jsonï¼ˆå¯é€‰ json_path æå–å­æ•°æ®ï¼‰
- file/pdf/image/etc: ä¸‹è½½å•ä¸ªæ–‡ä»¶
"""
import json
import time
from dataclasses import dataclass, field
from typing import Any, AsyncGenerator, Iterable, Optional

from loguru import logger

from src.agent.schemas import AgentRequest
from src.config import settings
from src.agent.chat.service import ChatService
from src.agent.config.service import AgentConfigService
from src.analytics.service import log_context_access

# Anthropic å®˜æ–¹ bash å·¥å…·
BASH_TOOL = {"type": "bash_20250124", "name": "bash"}


@dataclass
class SandboxFile:
    """æ²™ç›’ä¸­çš„æ–‡ä»¶"""
    path: str               # æ²™ç›’ä¸­çš„è·¯å¾„ï¼Œå¦‚ /workspace/data.json
    content: str | None = None      # æ–‡æœ¬å†…å®¹ï¼ˆJSON/æ–‡æœ¬æ–‡ä»¶ï¼‰
    s3_key: str | None = None       # S3 keyï¼ˆäºŒè¿›åˆ¶æ–‡ä»¶éœ€è¦ä¸‹è½½ï¼‰
    content_type: str = "application/octet-stream"


@dataclass
class SandboxData:
    """æ²™ç›’æ•°æ®"""
    files: list[SandboxFile] = field(default_factory=list)
    node_type: str = "json"
    root_node_id: str = ""
    root_node_name: str = ""
    # ç”¨äºå¤šæ–‡ä»¶å›å†™ï¼šnode_id -> {sandbox_path, node_type}
    node_path_map: dict = field(default_factory=dict)


class AgentService:
    """Agent æ ¸å¿ƒé€»è¾‘"""

    def __init__(self, anthropic_client=None):
        self._anthropic = anthropic_client or _default_anthropic_client()

    async def execute_task_sync(
        self,
        agent_id: str,
        task_content: str,
        user_id: str,
        node_service,
        sandbox_service,
        s3_service=None,
        agent_config_service=None,
        max_iterations: int = 15,
    ) -> dict:
        """
        éæµå¼æ‰§è¡Œ Agent ä»»åŠ¡ï¼ˆç”¨äº Schedule Agentï¼‰
        
        å¤ç”¨ stream_events çš„æ ¸å¿ƒé€»è¾‘ï¼Œä½†ä¸éœ€è¦æµå¼è¾“å‡ºå’ŒèŠå¤©å†å²ã€‚
        
        Args:
            agent_id: Agent ID
            task_content: ä»»åŠ¡å†…å®¹ï¼ˆæ¥è‡ª agent.task_contentï¼‰
            user_id: ç”¨æˆ· IDï¼ˆagent çš„ ownerï¼‰
            node_service: ContentNodeService
            sandbox_service: SandboxService
            s3_service: S3Serviceï¼ˆå¯é€‰ï¼‰
            agent_config_service: AgentConfigService
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            
        Returns:
            dict with execution results
        """
        import time
        from src.config import settings
        
        result = {
            "status": "success",
            "output_summary": "",
            "tool_calls": [],
            "updated_nodes": [],
        }
        
        try:
            # ========== 1. è·å– Agent é…ç½® ==========
            if not agent_config_service:
                return {"status": "failed", "error": "agent_config_service is required"}
            
            agent = agent_config_service.get_agent(agent_id)
            if not agent:
                return {"status": "failed", "error": f"Agent not found: {agent_id}"}
            
            if agent.user_id != user_id:
                return {"status": "failed", "error": "Unauthorized access to agent"}
            
            logger.info(f"[ScheduleAgent] Executing agent: {agent.name} (id={agent_id})")
            
            # ========== 2. æ”¶é›† bash tools ==========
            bash_tools: list[dict] = []
            for access in agent.accesses:
                if access.terminal:
                    bash_tools.append({
                        "node_id": access.node_id,
                        "json_path": (access.json_path or "").strip(),
                        "readonly": access.terminal_readonly,
                    })
                    logger.info(f"[ScheduleAgent] Found bash access: node_id={access.node_id}")
            
            use_bash = len(bash_tools) > 0
            sandbox_readonly = all(tool["readonly"] for tool in bash_tools) if bash_tools else True
            
            # ========== 3. å‡†å¤‡æ²™ç›’æ•°æ® ==========
            sandbox_data: SandboxData | None = None
            sandbox_session_id = None
            node_path_map: dict = {}
            
            if use_bash and node_service:
                all_files: list[SandboxFile] = []
                primary_node_type = "folder"
                primary_node_id = ""
                primary_node_name = ""
                
                for i, tool in enumerate(bash_tools):
                    try:
                        data = await prepare_sandbox_data(
                            node_service=node_service,
                            node_id=tool["node_id"],
                            json_path=tool["json_path"],
                            user_id=user_id,
                        )
                        logger.info(f"[ScheduleAgent] Prepared sandbox data: node_id={tool['node_id']}, files={len(data.files)}")
                        all_files.extend(data.files)
                        
                        if data.files:
                            main_path = data.files[0].path
                            node_path_map[tool["node_id"]] = {
                                "path": main_path,
                                "node_type": data.node_type,
                                "json_path": tool["json_path"],
                                "readonly": tool["readonly"],
                            }
                        
                        if i == 0:
                            primary_node_type = data.node_type
                            primary_node_id = data.root_node_id
                            primary_node_name = data.root_node_name
                    except Exception as e:
                        logger.warning(f"[ScheduleAgent] Failed to prepare sandbox data: {e}")
                
                sandbox_data = SandboxData(
                    files=all_files,
                    node_type=primary_node_type if len(bash_tools) == 1 else "multi",
                    root_node_id=primary_node_id,
                    root_node_name=primary_node_name,
                    node_path_map=node_path_map,
                )
            
            # ========== 4. å¯åŠ¨æ²™ç›’ ==========
            if use_bash and sandbox_service:
                sandbox_session_id = f"schedule-{int(time.time() * 1000)}"
                
                if sandbox_data and sandbox_data.node_type == "json" and len(bash_tools) == 1:
                    json_content = {}
                    if sandbox_data.files:
                        try:
                            json_content = json.loads(sandbox_data.files[0].content or "{}")
                        except:
                            json_content = {}
                    start_result = await sandbox_service.start(
                        session_id=sandbox_session_id,
                        data=json_content,
                        readonly=sandbox_readonly,
                    )
                else:
                    start_result = await sandbox_service.start_with_files(
                        session_id=sandbox_session_id,
                        files=sandbox_data.files if sandbox_data else [],
                        readonly=sandbox_readonly,
                        s3_service=s3_service,
                    )
                
                if not start_result.get("success"):
                    return {"status": "failed", "error": start_result.get("error", "Failed to start sandbox")}
                
                logger.info(f"[ScheduleAgent] Sandbox started: {sandbox_session_id}")
            
            # ========== 5. æ„å»º Claude è¯·æ±‚ ==========
            tools = [BASH_TOOL] if use_bash else []
            
            if use_bash and sandbox_data:
                node_type = sandbox_data.node_type
                if node_type == "json":
                    system_prompt = "You are an AI agent specialized in JSON data processing. Use the bash tool to view and manipulate data in /workspace/data.json."
                elif node_type == "folder":
                    system_prompt = "You are an AI agent with access to a file system. Use the bash tool to explore and manipulate files in /workspace/."
                elif node_type == "multi":
                    system_prompt = "You are an AI agent with access to multiple files and folders. Use the bash tool to explore and manipulate files in /workspace/."
                else:
                    system_prompt = f"You are an AI agent with access to a {node_type} file. Use the bash tool to analyze the file in /workspace/."
            else:
                system_prompt = "You are an AI agent, a helpful assistant."
            
            # æ„å»ºç”¨æˆ·æ¶ˆæ¯
            user_content = task_content
            if use_bash and sandbox_data:
                mode_str = "âš ï¸ åªè¯»æ¨¡å¼" if sandbox_readonly else "âœï¸ å¯è¯»å†™æ¨¡å¼"
                context_prefix = (
                    f"[æ•°æ®ä¸Šä¸‹æ–‡]\n"
                    f"å·¥ä½œç›®å½•: /workspace/\n"
                    f"æ–‡ä»¶æ•°é‡: {len(sandbox_data.files)}\n"
                    f"æƒé™: {mode_str}\n"
                    f"[ä»»åŠ¡]\n"
                )
                user_content = context_prefix + task_content
            
            messages = [{"role": "user", "content": user_content}]
            
            # ========== 6. è°ƒç”¨ Claude (éæµå¼å¾ªç¯) ==========
            iterations = 0
            all_text_outputs = []
            
            while iterations < max_iterations:
                iterations += 1
                logger.info(f"[ScheduleAgent] Claude iteration {iterations}")
                
                try:
                    response = await self._anthropic.messages.create(
                        model=settings.ANTHROPIC_MODEL,
                        max_tokens=4096,
                        system=system_prompt,
                        tools=tools,
                        messages=messages,
                    )
                    
                    stop_reason = response.stop_reason
                    content_blocks = response.content
                    
                    # å¤„ç†å“åº”å†…å®¹
                    tool_uses = []
                    response_content = []
                    
                    for block in content_blocks:
                        block_type = getattr(block, "type", None)
                        if block_type == "text":
                            text = getattr(block, "text", "")
                            all_text_outputs.append(text)
                            response_content.append({"type": "text", "text": text})
                        elif block_type == "tool_use":
                            tool_uses.append({
                                "id": getattr(block, "id", ""),
                                "name": getattr(block, "name", ""),
                                "input": getattr(block, "input", {}),
                            })
                            response_content.append({
                                "type": "tool_use",
                                "id": getattr(block, "id", ""),
                                "name": getattr(block, "name", ""),
                                "input": getattr(block, "input", {}),
                            })
                    
                    logger.info(f"[ScheduleAgent] Claude response: stop_reason={stop_reason}, tool_uses={len(tool_uses)}")
                    
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸ
                    if not tool_uses:
                        break
                    
                    # æ‰§è¡Œå·¥å…·
                    tool_results = []
                    for tool in tool_uses:
                        tool_name = tool.get("name", "")
                        tool_input = tool.get("input", {})
                        
                        if tool_name == "bash" and use_bash and sandbox_service:
                            command = tool_input.get("command", "")
                            logger.info(f"[ScheduleAgent] Executing bash: {command[:100]}")
                            
                            exec_result = await sandbox_service.exec(sandbox_session_id, command)
                            if exec_result.get("success"):
                                output = exec_result.get("output", "")
                                result["tool_calls"].append({
                                    "command": command,
                                    "output": output[:500],
                                    "success": True,
                                })
                            else:
                                output = exec_result.get("error", "")
                                result["tool_calls"].append({
                                    "command": command,
                                    "output": output[:500],
                                    "success": False,
                                })
                        else:
                            output = f"Unknown tool: {tool_name}"
                        
                        tool_results.append({
                            "type": "tool_result",
                            "tool_use_id": tool.get("id", ""),
                            "content": output,
                        })
                    
                    messages.append({"role": "assistant", "content": response_content})
                    messages.append({"role": "user", "content": tool_results})
                    
                    if stop_reason == "end_turn":
                        break
                        
                except Exception as e:
                    logger.error(f"[ScheduleAgent] Claude error: {e}")
                    result["status"] = "failed"
                    result["error"] = str(e)
                    break
            
            # ========== 7. å›å†™æ•°æ®åˆ°æ•°æ®åº“ ==========
            if use_bash and sandbox_service and sandbox_session_id and not sandbox_readonly:
                if sandbox_data and sandbox_data.node_path_map and node_service:
                    for node_id, info in sandbox_data.node_path_map.items():
                        if info.get("readonly"):
                            continue
                        
                        node_type = info.get("node_type", "")
                        sandbox_path = info.get("path", "")
                        json_path_config = info.get("json_path", "")
                        
                        if node_type not in ("json", "markdown"):
                            continue
                        
                        try:
                            parse_json = (node_type == "json")
                            read_result = await sandbox_service.read_file(
                                sandbox_session_id, sandbox_path, parse_json=parse_json
                            )
                            
                            if not read_result.get("success"):
                                continue
                            
                            sandbox_content = read_result.get("content")
                            node = node_service.get_by_id(node_id, user_id)
                            if not node:
                                continue
                            
                            if node_type == "json":
                                if json_path_config:
                                    updated_data = merge_data_by_path(
                                        node.content or {}, json_path_config, sandbox_content
                                    )
                                else:
                                    updated_data = sandbox_content
                                
                                node_service.update_node(
                                    node_id=node_id,
                                    user_id=user_id,
                                    content=updated_data,
                                )
                            elif node_type == "markdown":
                                await node_service.update_markdown_content(
                                    node_id=node_id,
                                    user_id=user_id,
                                    content=sandbox_content,
                                )
                            
                            result["updated_nodes"].append({
                                "nodeId": node_id,
                                "nodeName": node.name,
                            })
                            logger.info(f"[ScheduleAgent] Wrote back data to node: {node_id}")
                            
                        except Exception as e:
                            logger.warning(f"[ScheduleAgent] Failed to write back: {e}")
                
                # åœæ­¢æ²™ç›’
                await sandbox_service.stop(sandbox_session_id)
                logger.info(f"[ScheduleAgent] Sandbox stopped")
            elif sandbox_session_id and sandbox_service:
                await sandbox_service.stop(sandbox_session_id)
            
            # ========== 8. è¿”å›ç»“æœ ==========
            result["output_summary"] = "\n".join(all_text_outputs)[:2000]
            logger.info(f"[ScheduleAgent] Execution completed: {result['status']}")
            return result
            
        except Exception as e:
            logger.error(f"[ScheduleAgent] Execution failed: {e}")
            return {"status": "failed", "error": str(e)}

    async def stream_events(
        self,
        request: AgentRequest,
        current_user,
        node_service,  # ContentNodeServiceï¼Œç”¨äºè·å– content_nodes æ•°æ®
        tool_service,
        sandbox_service,
        chat_service: Optional[ChatService] = None,
        s3_service=None,  # S3Serviceï¼Œç”¨äºä¸‹è½½æ–‡ä»¶
        agent_config_service: Optional[AgentConfigService] = None,  # æ–°ç‰ˆ agent é…ç½®æœåŠ¡
        max_iterations: int = 15,
    ) -> AsyncGenerator[dict, None]:
        """
        å¤„ç† Agent è¯·æ±‚çš„ä¸»å…¥å£
        
        ä¼˜å…ˆçº§ï¼š
        1. å¦‚æœæœ‰ agent_idï¼Œä» agent_access è¡¨è¯»å–é…ç½®ï¼ˆæ–°ç‰ˆï¼‰
        2. å¦åˆ™ï¼Œä» active_tool_ids æŸ¥ tool è¡¨ï¼ˆæ—§ç‰ˆå…¼å®¹ï¼‰
        """
        
        # ========== 1. è§£æé…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨æ–°ç‰ˆ agent_access ==========
        bash_tools: list[dict] = []  # [{node_id, json_path, readonly}, ...]
        
        logger.info(f"[Agent DEBUG] agent_id={request.agent_id}, active_tool_ids={request.active_tool_ids}")
        
        # æ–°ç‰ˆï¼šå¦‚æœæœ‰ agent_idï¼Œä» agent_bash è¡¨è¯»å–é…ç½®
        if request.agent_id and current_user and agent_config_service:
            try:
                agent = agent_config_service.get_agent(request.agent_id)
                if agent and agent.user_id == current_user.user_id:
                    logger.info(f"[Agent] Found agent config: id={agent.id}, bash_accesses={len(agent.bash_accesses)}")
                    # æ”¶é›†æ‰€æœ‰ Bash è®¿é—®æƒé™ï¼ˆæ–°ç‰ˆæ¶æ„ä¸‹æ‰€æœ‰ bash_accesses éƒ½æ˜¯ç»ˆç«¯è®¿é—®ï¼‰
                    for bash in agent.bash_accesses:
                        bash_tools.append({
                            "node_id": bash.node_id,
                            "json_path": (bash.json_path or "").strip(),
                            "readonly": bash.readonly,
                        })
                        logger.info(f"[Agent] Found bash access from agent_bash: node_id={bash.node_id}")
                    logger.info(f"[Agent] Total bash accesses collected: {len(bash_tools)}")
                else:
                    logger.warning(f"[Agent] Agent not found or unauthorized: {request.agent_id}")
            except Exception as e:
                logger.warning(f"[Agent] Failed to get agent config: {e}")
        
        # æ—§ç‰ˆå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ä» agent_access è·å–åˆ°é…ç½®ï¼Œfallback åˆ° tool è¡¨
        if len(bash_tools) == 0 and request.active_tool_ids and current_user and tool_service:
            logger.info(f"[Agent] Fallback to tool table for bash config")
            for tool_id in request.active_tool_ids:
                try:
                    tool = tool_service.get_by_id(tool_id)
                    logger.info(f"[Agent DEBUG] tool_id={tool_id}, tool={tool}")
                    if tool:
                        logger.info(f"[Agent DEBUG] tool.user_id={tool.user_id}, current_user.user_id={current_user.user_id}")
                        logger.info(f"[Agent DEBUG] tool.type={tool.type}, tool.node_id={tool.node_id}")
                    if tool and tool.user_id == current_user.user_id:
                        tool_type = (tool.type or "").strip()
                        if tool_type in ("shell_access", "shell_access_readonly"):
                            bash_tools.append({
                                "node_id": tool.node_id,
                                    "json_path": (tool.json_path or "").strip(),
                                    "readonly": tool_type == "shell_access_readonly",
                            })
                            logger.info(f"[Agent] Found bash tool from tool table: node_id={tool.node_id}")
                except Exception as e:
                    logger.warning(f"[Agent] Failed to get tool {tool_id}: {e}")
        
        # ========== 2. Chat persistence (best-effort) ==========
        persisted_session_id: str | None = None
        created_session = False
        should_persist = current_user is not None and chat_service is not None
        
        logger.info(f"[Chat Persist] should_persist={should_persist}, current_user={current_user is not None}, chat_service={chat_service is not None}")
        
        if should_persist:
            try:
                persisted_session_id, created_session = chat_service.ensure_session(
                    user_id=current_user.user_id,
                    session_id=request.session_id,
                    agent_id=request.agent_id,
                    mode="agent",
                )
                logger.info(f"[Chat Persist] Session ready: id={persisted_session_id}, created={created_session}")
                # å¦‚æœæ˜¯æ–°å»ºçš„ sessionï¼Œå…ˆè®¾ç½® titleï¼Œå†é€šçŸ¥å‰ç«¯
                if created_session and persisted_session_id:
                    try:
                        chat_service.maybe_set_title_on_first_message(
                            user_id=current_user.user_id,
                            session_id=persisted_session_id,
                            first_message=request.prompt,
                        )
                        logger.info(f"[Chat Persist] Session title set for {persisted_session_id}")
                    except Exception as e:
                        logger.warning(f"[Chat Persist] Failed to set session title: {e}")
                    yield {"type": "session", "sessionId": persisted_session_id}
            except Exception as e:
                logger.error(f"[Chat Persist] Failed to ensure session: {e}")
                persisted_session_id = None
                should_persist = False

        # åŠ è½½å†å²æ¶ˆæ¯
        messages: list[dict[str, Any]] = []
        if should_persist and persisted_session_id:
            try:
                history = chat_service.load_history_for_llm(
                    user_id=current_user.user_id, session_id=persisted_session_id, limit=60
                )
                messages.extend(history)
            except Exception:
                if request.chatHistory:
                    for item in request.chatHistory:
                        messages.append({"role": item.role, "content": item.content})
        else:
            if request.chatHistory:
                for item in request.chatHistory:
                    messages.append({"role": item.role, "content": item.content})

        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        if should_persist and persisted_session_id:
            try:
                chat_service.add_user_message(session_id=persisted_session_id, content=request.prompt)
                logger.info(f"[Chat Persist] User message saved to session {persisted_session_id}")
            except Exception as e:
                logger.error(f"[Chat Persist] Failed to save user message: {e}")

        # ========== 3. å¦‚æœæœ‰ bash toolsï¼Œå‡†å¤‡æ²™ç›’æ•°æ®ã€å¯åŠ¨æ²™ç›’ ==========
        use_bash = len(bash_tools) > 0
        sandbox_data: SandboxData | None = None
        sandbox_session_id = None
        # å¦‚æœä»»æ„ä¸€ä¸ª access ä¸æ˜¯ readonlyï¼Œåˆ™æ•´ä¸ªæ²™ç›’ä¸æ˜¯ readonly
        sandbox_readonly = all(tool["readonly"] for tool in bash_tools) if bash_tools else True
        
        if use_bash and node_service and current_user:
            # æ”¶é›†æ‰€æœ‰ bash access çš„æ–‡ä»¶
            all_files: list[SandboxFile] = []
            primary_node_type = "folder"  # é»˜è®¤ç±»å‹
            primary_node_id = ""
            primary_node_name = ""
            # è¿½è¸ªæ¯ä¸ª node å¯¹åº”çš„æ²™ç›’è·¯å¾„å’Œç±»å‹ï¼Œç”¨äºå›å†™
            node_path_map: dict = {}  # {node_id: {path, node_type, json_path}}
            
            for i, tool in enumerate(bash_tools):
                try:
                    data = await prepare_sandbox_data(
                        node_service=node_service,
                        node_id=tool["node_id"],
                        json_path=tool["json_path"],
                        user_id=current_user.user_id,
                    )
                    logger.info(f"[Agent] Prepared sandbox data for access {i+1}/{len(bash_tools)}: "
                               f"node_id={tool['node_id']}, type={data.node_type}, files={len(data.files)}")
                    all_files.extend(data.files)
                    
                    # Log context access (data egress tracking)
                    await log_context_access(
                        node_id=tool["node_id"],
                        node_type=data.node_type,
                        node_name=data.root_node_name,
                        user_id=current_user.user_id if current_user else None,
                        agent_id=request.agent_id,
                        session_id=request.session_id,
                    )
                    
                    # è®°å½•è·¯å¾„æ˜ å°„ï¼ˆç”¨äºå›å†™ï¼‰
                    if data.files:
                        # å¯¹äº JSON å’Œå•æ–‡ä»¶ï¼Œè®°å½•ä¸»æ–‡ä»¶è·¯å¾„
                        main_path = data.files[0].path
                        node_path_map[tool["node_id"]] = {
                            "path": main_path,
                            "node_type": data.node_type,
                            "json_path": tool["json_path"],
                            "readonly": tool["readonly"],
                        }
                    
                    # ç¬¬ä¸€ä¸ª access å†³å®šä¸»ç±»å‹
                    if i == 0:
                        primary_node_type = data.node_type
                        primary_node_id = data.root_node_id
                        primary_node_name = data.root_node_name
                except Exception as e:
                    logger.warning(f"[Agent] Failed to prepare sandbox data for node {tool['node_id']}: {e}")
            
            sandbox_data = SandboxData(
                files=all_files,
                node_type=primary_node_type if len(bash_tools) == 1 else "multi",  # å¤šä¸ªæ—¶æ ‡è®°ä¸º multi
                root_node_id=primary_node_id,
                root_node_name=primary_node_name,
                node_path_map=node_path_map,
            )
            logger.info(f"[Agent] Total sandbox files: {len(all_files)} from {len(bash_tools)} accesses, path_map={list(node_path_map.keys())}")

        if use_bash and sandbox_service:
            sandbox_session_id = f"agent-{int(time.time() * 1000)}"
            
            # æ ¹æ®èŠ‚ç‚¹ç±»å‹é€‰æ‹©å¯åŠ¨æ–¹å¼
            if sandbox_data and sandbox_data.node_type == "json" and len(bash_tools) == 1:
                # å•ä¸ª JSON èŠ‚ç‚¹ï¼šä½¿ç”¨ç°æœ‰çš„ data å‚æ•°
                json_content = {}
                if sandbox_data.files:
                    try:
                        json_content = json.loads(sandbox_data.files[0].content or "{}")
                    except:
                        json_content = {}
                start_result = await sandbox_service.start(
                    session_id=sandbox_session_id,
                    data=json_content,
                    readonly=sandbox_readonly,
                )
            else:
                # Folder/File/å¤šä¸ªèŠ‚ç‚¹ï¼šä½¿ç”¨ files å‚æ•°
                start_result = await sandbox_service.start_with_files(
                    session_id=sandbox_session_id,
                    files=sandbox_data.files if sandbox_data else [],
                    readonly=sandbox_readonly,
                    s3_service=s3_service,
                )
            
            if not start_result.get("success"):
                err_msg = start_result.get("error", "Failed to start sandbox")
                yield {"type": "error", "message": err_msg}
                if should_persist and persisted_session_id:
                    try:
                        chat_service.add_assistant_message(
                            session_id=persisted_session_id,
                            content=err_msg,
                            parts=[{"type": "text", "content": err_msg}],
                        )
                    except Exception:
                        pass
                return
            
            yield {"type": "status", "message": "Sandbox ready"}

        # ========== 4. æ„å»º Claude è¯·æ±‚ ==========
        tools = [BASH_TOOL] if use_bash else []
        
        # æ ¹æ®èŠ‚ç‚¹ç±»å‹æ„å»ºç³»ç»Ÿæç¤º
        if use_bash and sandbox_data:
            node_type = sandbox_data.node_type
            if node_type == "json":
                system_prompt = "You are an AI agent specialized in JSON data processing. Use the bash tool to view and manipulate data in /workspace/data.json."
            elif node_type == "folder":
                system_prompt = "You are an AI agent with access to a file system. Use the bash tool to explore and manipulate files in /workspace/."
            elif node_type == "multi":
                system_prompt = "You are an AI agent with access to multiple files and folders. Use the bash tool to explore and manipulate files in /workspace/."
            else:
                system_prompt = f"You are an AI agent with access to a {node_type} file. Use the bash tool to analyze the file in /workspace/."
        else:
            system_prompt = "You are an AI agent, a helpful assistant."

        # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼šå¦‚æœä½¿ç”¨ bashï¼Œæ·»åŠ æ•°æ®ä¸Šä¸‹æ–‡
        user_content = request.prompt
        if use_bash and sandbox_data:
            node_type = sandbox_data.node_type
            node_path_map = sandbox_data.node_path_map or {}
            
            # ç”Ÿæˆè¯¦ç»†çš„æƒé™æ¸…å•
            def build_access_list() -> str:
                lines = []
                for tool in bash_tools:
                    path_info = node_path_map.get(tool["node_id"], {})
                    path = path_info.get("path", "/workspace/unknown")
                    node_name = path.split("/")[-1] if path else "unknown"
                    mode = "ğŸ‘ï¸ View Only" if tool["readonly"] else "âœï¸ Editable"
                    lines.append(f"  - {path} ({mode})")
                return "\n".join(lines) if lines else "  - /workspace/ (unknown)"
            
            if node_type == "json" and len(bash_tools) == 1:
                json_path = bash_tools[0]["json_path"] or "/"
                mode_str = "âš ï¸ åªè¯»æ¨¡å¼ - ä¿®æ”¹ä¸ä¼šè¢«ä¿å­˜" if sandbox_readonly else "âœï¸ å¯è¯»å†™æ¨¡å¼"
                context_prefix = (
                    f"[æ•°æ®ä¸Šä¸‹æ–‡]\n"
                    f"èŠ‚ç‚¹ç±»å‹: JSON\n"
                    f"æ•°æ®æ–‡ä»¶: /workspace/data.json\n"
                    f"JSON è·¯å¾„: {json_path}\n"
                    f"æƒé™: {mode_str}\n"
                    f"[ç”¨æˆ·æ¶ˆæ¯]\n"
                )
            elif node_type == "folder" and len(bash_tools) == 1:
                mode_str = "âš ï¸ åªè¯»æ¨¡å¼ - ä¿®æ”¹ä¸ä¼šè¢«ä¿å­˜" if sandbox_readonly else "âœï¸ å¯è¯»å†™æ¨¡å¼"
                context_prefix = (
                    f"[æ•°æ®ä¸Šä¸‹æ–‡]\n"
                    f"èŠ‚ç‚¹ç±»å‹: æ–‡ä»¶å¤¹\n"
                    f"æ–‡ä»¶å¤¹å: {sandbox_data.root_node_name}\n"
                    f"å·¥ä½œç›®å½•: /workspace/\n"
                    f"æ–‡ä»¶æ•°é‡: {len(sandbox_data.files)}\n"
                    f"æƒé™: {mode_str}\n"
                    f"[ç”¨æˆ·æ¶ˆæ¯]\n"
                )
            elif node_type == "multi" or len(bash_tools) > 1:
                # å¤šä¸ª access - æ˜¾ç¤ºè¯¦ç»†æƒé™æ¸…å•
                access_list = build_access_list()
                context_prefix = (
                    f"[æ•°æ®ä¸Šä¸‹æ–‡]\n"
                    f"å·¥ä½œç›®å½•: /workspace/\n"
                    f"æ€»æ–‡ä»¶æ•°: {len(sandbox_data.files)}\n\n"
                    f"ğŸ“‚ å¯è®¿é—®çš„èµ„æº:\n"
                    f"{access_list}\n\n"
                    f"âš ï¸ é‡è¦æç¤º:\n"
                    f"  - åªæœ‰ä¸Šè¿°è·¯å¾„çš„å†…å®¹ä¼šè¢«ä¿å­˜\n"
                    f"  - /workspace/ æ ¹ç›®å½•ä¸èƒ½åˆ›å»ºæ–°æ–‡ä»¶\n"
                    f"  - View Only è·¯å¾„çš„ä¿®æ”¹ä¸ä¼šè¢«æŒä¹…åŒ–\n"
                    f"[ç”¨æˆ·æ¶ˆæ¯]\n"
                )
            else:
                # file/pdf/image ç­‰å•ä¸ªæ–‡ä»¶
                file_name = sandbox_data.files[0].path.split("/")[-1] if sandbox_data.files else "file"
                mode_str = "âš ï¸ åªè¯»æ¨¡å¼ - ä¿®æ”¹ä¸ä¼šè¢«ä¿å­˜" if sandbox_readonly else "âœï¸ å¯è¯»å†™æ¨¡å¼"
                context_prefix = (
                    f"[æ•°æ®ä¸Šä¸‹æ–‡]\n"
                    f"èŠ‚ç‚¹ç±»å‹: {node_type}\n"
                    f"æ–‡ä»¶å: {file_name}\n"
                    f"æ–‡ä»¶è·¯å¾„: /workspace/{file_name}\n"
                    f"æƒé™: {mode_str}\n"
                    f"[ç”¨æˆ·æ¶ˆæ¯]\n"
                )
            user_content = context_prefix + request.prompt
        
        messages.append({"role": "user", "content": user_content})

        # ========== 5. è°ƒç”¨ Claude (æµå¼)ï¼Œå¤„ç†å·¥å…·è°ƒç”¨ ==========
        persisted_parts: list[dict[str, Any]] = []
        tool_index = 0
        iterations = 0

        while iterations < max_iterations:
            iterations += 1
            
            logger.info(f"[CLAUDE REQUEST] Iteration {iterations} (streaming)")
            logger.info(f"[CLAUDE DEBUG] system_prompt = {system_prompt}")
            logger.info(f"[CLAUDE DEBUG] tools = {json.dumps(tools, ensure_ascii=False)}")
            logger.info(f"[CLAUDE DEBUG] messages = {json.dumps(messages, ensure_ascii=False, default=str)[:2000]}")
            
            try:
                # ===== æµå¼è°ƒç”¨ Claude =====
                current_text_content = ""
                tool_uses: list[dict[str, Any]] = []
                current_tool: dict[str, Any] | None = None
                current_tool_input_json = ""
                stop_reason = None
                response_content: list[Any] = []
                
                async with self._anthropic.messages.stream(
                    model=settings.ANTHROPIC_MODEL,
                    max_tokens=4096,
                    system=system_prompt,
                    tools=tools,
                    messages=messages,
                ) as stream:
                    async for event in stream:
                        event_type = getattr(event, "type", None)
                        
                        if event_type == "content_block_start":
                            block = getattr(event, "content_block", None)
                            block_type = getattr(block, "type", None) if block else None
                            
                            if block_type == "text":
                                current_text_content = ""
                            elif block_type == "tool_use":
                                current_tool = {
                                    "id": getattr(block, "id", ""),
                                    "name": getattr(block, "name", ""),
                                    "input": {},
                                }
                                current_tool_input_json = ""
                        
                        elif event_type == "content_block_delta":
                            delta = getattr(event, "delta", None)
                            delta_type = getattr(delta, "type", None) if delta else None
                            
                            if delta_type == "text_delta":
                                text = getattr(delta, "text", "")
                                if text:
                                    current_text_content += text
                                    # å®æ—¶ yield æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µï¼
                                    yield {"type": "text_delta", "content": text}
                            
                            elif delta_type == "input_json_delta":
                                partial_json = getattr(delta, "partial_json", "")
                                if partial_json:
                                    current_tool_input_json += partial_json
                        
                        elif event_type == "content_block_stop":
                            if current_text_content:
                                # æ–‡æœ¬å—ç»“æŸï¼Œä¿å­˜å®Œæ•´æ–‡æœ¬
                                persisted_parts.append({"type": "text", "content": current_text_content})
                                response_content.append({"type": "text", "text": current_text_content})
                                current_text_content = ""
                            
                            if current_tool:
                                # å·¥å…·å—ç»“æŸï¼Œè§£æ JSON è¾“å…¥
                                try:
                                    current_tool["input"] = json.loads(current_tool_input_json) if current_tool_input_json else {}
                                except json.JSONDecodeError:
                                    current_tool["input"] = {"raw": current_tool_input_json}
                                
                                tool_uses.append(current_tool)
                                response_content.append({
                                    "type": "tool_use",
                                    "id": current_tool["id"],
                                    "name": current_tool["name"],
                                    "input": current_tool["input"],
                                })
                                current_tool = None
                                current_tool_input_json = ""
                        
                        elif event_type == "message_delta":
                            delta = getattr(event, "delta", None)
                            if delta:
                                stop_reason = getattr(delta, "stop_reason", None)
                
                logger.info(f"[CLAUDE RESPONSE] Iteration {iterations}: stop_reason={stop_reason}, tool_uses={len(tool_uses)}")
                
            except Exception as e:
                msg = str(e)
                logger.error(f"[CLAUDE ERROR] {msg}")
                yield {"type": "error", "message": msg}
                if should_persist and persisted_session_id:
                    try:
                        chat_service.add_assistant_message(
                            session_id=persisted_session_id,
                            content=msg,
                            parts=[{"type": "text", "content": msg}],
                        )
                    except Exception:
                        pass
                if sandbox_session_id and sandbox_service:
                    try:
                        await sandbox_service.stop(sandbox_session_id)
                    except Exception:
                        pass
                return

            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¾ªç¯
            if not tool_uses:
                break

            # æ‰§è¡Œå·¥å…·
            tool_results = []
            for tool in tool_uses:
                current_tool_index = tool_index
                tool_index += 1
                tool_name = tool.get("name", "")
                tool_input = tool.get("input", {})
                
                yield {
                    "type": "tool_start",
                    "toolId": current_tool_index,
                    "toolName": tool_name,
                    "toolInput": tool_input.get("command") if tool_name == "bash" else json.dumps(tool_input),
                }
                
                persisted_parts.append({
                    "type": "tool",
                    "toolId": str(current_tool_index),
                    "toolName": tool_name or "tool",
                    "toolInput": tool_input.get("command") if tool_name == "bash" else json.dumps(tool_input),
                    "toolStatus": "running",
                })

                success = True
                output = ""
                
                if tool_name == "bash" and use_bash and sandbox_service:
                    exec_result = await sandbox_service.exec(
                        sandbox_session_id, tool_input.get("command", "")
                    )
                    if exec_result.get("success"):
                        output = exec_result.get("output", "")
                    else:
                        success = False
                        output = exec_result.get("error", "")
                else:
                    output = f"Unknown tool: {tool_name}"
                    success = False

                yield {
                    "type": "tool_end",
                    "toolId": current_tool_index,
                    "toolName": tool_name,
                    "output": output[:500],
                    "success": success,
                }
                
                # æ›´æ–°æŒä¹…åŒ–çŠ¶æ€
                for i in range(len(persisted_parts) - 1, -1, -1):
                    p = persisted_parts[i]
                    if p.get("type") == "tool" and p.get("toolId") == str(current_tool_index):
                        p["toolStatus"] = "completed" if success else "error"
                        p["toolOutput"] = output[:500]
                        break

                tool_results.append({
                    "type": "tool_result",
                    "tool_use_id": tool.get("id", ""),
                    "content": output,
                    "is_error": not success,
                })

            messages.append({"role": "assistant", "content": response_content})
            messages.append({"role": "user", "content": tool_results})
            
            if stop_reason == "end_turn":
                break

        # ========== 6. ä¿å­˜ç»“æœã€æ¸…ç†æ²™ç›’ ==========
        logger.info(f"[Chat Persist] Attempting to save assistant message: should_persist={should_persist}, session_id={persisted_session_id}, parts_count={len(persisted_parts)}")
        if should_persist and persisted_session_id:
            try:
                for p in persisted_parts:
                    if p.get("type") == "tool" and p.get("toolStatus") == "running":
                        p["toolStatus"] = "completed"
                final_content = "\n\n".join(
                    [p.get("content", "") for p in persisted_parts if p.get("type") == "text"]
                ).strip()
                logger.info(f"[Chat Persist] Saving assistant message: content_length={len(final_content)}, parts={persisted_parts}")
                chat_service.add_assistant_message(
                    session_id=persisted_session_id, content=final_content, parts=persisted_parts
                )
                logger.info(f"[Chat Persist] Assistant message saved successfully!")
            except Exception as e:
                logger.error(f"[Chat Persist] Failed to save assistant message: {e}")
        else:
            logger.warning(f"[Chat Persist] Skipping assistant message save: should_persist={should_persist}, session_id={persisted_session_id}")

        if use_bash and sandbox_service and sandbox_session_id:
            # å›å†™ä¿®æ”¹çš„æ•°æ®åˆ°æ•°æ®åº“
            updated_nodes = []
            
            if sandbox_data and sandbox_data.node_path_map and node_service and current_user:
                # éå†æ‰€æœ‰éåªè¯»çš„ accessï¼Œå›å†™æ•°æ®
                for node_id, info in sandbox_data.node_path_map.items():
                    if info.get("readonly"):
                        continue  # è·³è¿‡åªè¯»çš„
                    
                    node_type = info.get("node_type", "")
                    sandbox_path = info.get("path", "")
                    json_path = info.get("json_path", "")
                    
                    # æ”¯æŒå›å†™çš„èŠ‚ç‚¹ç±»å‹ï¼šjson, markdown
                    if node_type not in ("json", "markdown"):
                        logger.info(f"[Agent] Skipping write-back for unsupported node type: {node_id} (type={node_type})")
                        continue
                    
                    try:
                        # ä»æ²™ç›’è¯»å–æ–‡ä»¶
                        # JSON æ–‡ä»¶éœ€è¦è§£æï¼Œmarkdown æ–‡ä»¶ä¿æŒåŸæ ·
                        parse_json = (node_type == "json")
                        read_result = await sandbox_service.read_file(
                            sandbox_session_id, 
                            sandbox_path, 
                            parse_json=parse_json
                        )
                        
                        if not read_result.get("success"):
                            logger.warning(f"[Agent] Failed to read file from sandbox: {sandbox_path}")
                            continue
                        
                        sandbox_content = read_result.get("content")
                        
                        # è·å–åŸèŠ‚ç‚¹æ•°æ®
                        node = node_service.get_by_id(node_id, current_user.user_id)
                        if not node:
                            continue
                        
                        # æ ¹æ®èŠ‚ç‚¹ç±»å‹é€‰æ‹©ä¸åŒçš„æ›´æ–°æ–¹å¼
                        if node_type == "json":
                            # JSON ç±»å‹ï¼šåˆå¹¶æ•°æ®ï¼ˆå¦‚æœæœ‰ json_pathï¼‰
                            if json_path:
                                updated_data = merge_data_by_path(
                                    node.content or {}, json_path, sandbox_content
                                )
                            else:
                                updated_data = sandbox_content
                            
                            # å†™å›æ•°æ®åº“ï¼ˆcontent å­—æ®µï¼‰
                            node_service.update_node(
                                node_id=node_id,
                                user_id=current_user.user_id,
                                content=updated_data,
                            )
                        elif node_type == "markdown":
                            # markdown ç±»å‹ï¼šä¸Šä¼ åˆ° S3
                            await node_service.update_markdown_content(
                                node_id=node_id,
                                user_id=current_user.user_id,
                                content=sandbox_content,
                            )
                        
                        updated_nodes.append({
                            "nodeId": node_id,
                            "nodeName": node.name,
                            "modifiedPath": json_path,
                        })
                        logger.info(f"[Agent] Successfully wrote back data to node: {node_id}")
                        
                    except Exception as e:
                        logger.warning(f"[Agent] Failed to write back data for node {node_id}: {e}")
            
            # è¿”å›ç»“æœ
            if updated_nodes:
                yield {
                    "type": "result",
                    "success": True,
                    "updatedNodes": updated_nodes,
                }
            else:
                yield {"type": "result", "success": True}
            
            await sandbox_service.stop(sandbox_session_id)
        else:
            yield {"type": "result", "success": True}


# ========== å·¥å…·å‡½æ•° ==========

async def prepare_sandbox_data(
    node_service,
    node_id: str,
    json_path: str | None,
    user_id: str,
) -> SandboxData:
    """
    ç»Ÿä¸€çš„æ²™ç›’æ•°æ®å‡†å¤‡å‡½æ•°
    
    æ ¹æ®èŠ‚ç‚¹ç±»å‹è¿”å›ä¸åŒçš„æ²™ç›’æ•°æ®ï¼š
    - json: å¯¼å‡º content ä¸º data.jsonï¼ˆå¯é€‰ json_path æå–å­æ•°æ®ï¼‰
    - folder: é€’å½’è·å–å­æ–‡ä»¶åˆ—è¡¨
    - github_repo: ä» S3 ç›®å½•ä¸‹è½½æ‰€æœ‰æ–‡ä»¶ï¼ˆå•èŠ‚ç‚¹æ¨¡å¼ï¼‰
    - file/pdf/image/etc: å•ä¸ªæ–‡ä»¶ä¿¡æ¯
    """
    node = node_service.get_by_id(node_id, user_id)
    if not node:
        raise ValueError(f"Node not found: {node_id}")
    
    logger.info(f"[prepare_sandbox_data] node.id={node.id}, type={node.type}, name={node.name}")
    
    files: list[SandboxFile] = []
    node_type = node.type or "json"
    
    if node_type == "github_repo":
        # GitHub Repo èŠ‚ç‚¹ï¼ˆå•èŠ‚ç‚¹æ¨¡å¼ï¼‰ï¼šä» content.files è¯»å–æ–‡ä»¶åˆ—è¡¨
        # æ¯ä¸ªæ–‡ä»¶éƒ½æœ‰ s3_keyï¼Œç”¨äºä» S3 ä¸‹è½½
        content = node.content or {}
        file_list = content.get("files", [])
        repo_name = content.get("repo", node.name or "repo")
        
        logger.info(f"[prepare_sandbox_data] GitHub repo node, file_count={len(file_list)}")
        
        for file_info in file_list:
            file_path = file_info.get("path", "")
            s3_key = file_info.get("s3_key", "")
            
            if not file_path or not s3_key:
                continue
            
            # ä¿æŒ repo å†…çš„ç›®å½•ç»“æ„
            files.append(SandboxFile(
                path=f"/workspace/{repo_name}/{file_path}",
                s3_key=s3_key,
                content_type="text/plain",  # GitHub æ–‡ä»¶éƒ½æ˜¯æ–‡æœ¬
            ))
    
    elif node_type == "json":
        # JSON èŠ‚ç‚¹ï¼šå¯¼å‡º content ä¸º data.json
        content = node.content or {}
        if json_path:
            content = extract_data_by_path(content, json_path)
        
        files.append(SandboxFile(
            path="/workspace/data.json",
            content=json.dumps(content, ensure_ascii=False, indent=2),
            content_type="application/json",
        ))
        logger.info(f"[prepare_sandbox_data] JSON node, content size={len(str(content))}")
        
    elif node_type == "folder":
        # Folder èŠ‚ç‚¹ï¼šé€’å½’è·å–æ‰€æœ‰å­æ–‡ä»¶
        # ä½¿ç”¨ list_descendants è·å–æ‰€æœ‰å­å­™èŠ‚ç‚¹
        children = node_service.list_descendants(node.project_id, node_id)
        logger.info(f"[prepare_sandbox_data] Folder node, children count={len(children)}")
        
        for child in children:
            if child.type == "folder":
                continue  # è·³è¿‡å­æ–‡ä»¶å¤¹æœ¬èº«ï¼Œåªå¤„ç†æ–‡ä»¶
            
            # è®¡ç®—ç›¸å¯¹è·¯å¾„
            relative_path = child.id_path.replace(node.id_path, "").lstrip("/")
            if not relative_path:
                relative_path = child.name
            
            if child.type == "json":
                # JSON å­èŠ‚ç‚¹ï¼šå¯¼å‡ºä¸º .json æ–‡ä»¶
                files.append(SandboxFile(
                    path=f"/workspace/{relative_path}.json",
                    content=json.dumps(child.content or {}, ensure_ascii=False, indent=2),
                    content_type="application/json",
                ))
            elif child.s3_key:
                # å…¶ä»–æ–‡ä»¶ç±»å‹ï¼šè®°å½• S3 keyï¼Œç”±æ²™ç›’æœåŠ¡ä¸‹è½½
                files.append(SandboxFile(
                    path=f"/workspace/{relative_path}",
                    s3_key=child.s3_key,
                    content_type=child.mime_type or "application/octet-stream",
                ))
    else:
        # å…¶ä»–æ–‡ä»¶ç±»å‹ï¼ˆpdf, image, file, markdown, notion_page ç­‰ï¼‰
        file_name = node.name or "file"
        
        if node.content and isinstance(node.content, (dict, list)):
            # å¦‚æœæœ‰ JSON contentï¼Œå¯¼å‡ºä¸º JSON
            files.append(SandboxFile(
                path=f"/workspace/{file_name}.json",
                content=json.dumps(node.content, ensure_ascii=False, indent=2),
                content_type="application/json",
            ))
        elif node.s3_key:
            # S3 æ–‡ä»¶ï¼ˆmarkdown, image, pdf ç­‰ï¼‰
            files.append(SandboxFile(
                path=f"/workspace/{file_name}",
                s3_key=node.s3_key,
                content_type=node.mime_type or "application/octet-stream",
            ))
        else:
            logger.warning(f"[prepare_sandbox_data] Node has no content or s3_key: {node_id}")
    
    return SandboxData(
        files=files,
        node_type=node_type,
        root_node_id=node.id,
        root_node_name=node.name or "",
    )


def extract_data_by_path(data, json_path: str):
    """ä» JSON æ•°æ®ä¸­æå–æŒ‡å®šè·¯å¾„çš„èŠ‚ç‚¹"""
    if not json_path or json_path == "/" or json_path == "":
        return data

    segments = [segment for segment in json_path.split("/") if segment]
    current = data
    for segment in segments:
        if current is None:
            return None
        if isinstance(current, list):
            try:
                index = int(segment)
            except (TypeError, ValueError):
                return None
            if index < 0 or index >= len(current):
                return None
            current = current[index]
        elif isinstance(current, dict):
            current = current.get(segment)
        else:
            return None
    return current


def merge_data_by_path(original_data, json_path: str, new_node_data):
    """å°†æ–°æ•°æ®åˆå¹¶å›åŸæ•°æ®çš„æŒ‡å®šè·¯å¾„"""
    if not json_path or json_path == "/" or json_path == "":
        return new_node_data

    result = json.loads(json.dumps(original_data))
    segments = [segment for segment in json_path.split("/") if segment]

    current = result
    for segment in segments[:-1]:
        if isinstance(current, list):
            current = current[int(segment)]
        elif isinstance(current, dict):
            current = current[segment]

    last_segment = segments[-1]
    if isinstance(current, list):
        current[int(last_segment)] = new_node_data
    elif isinstance(current, dict):
        current[last_segment] = new_node_data

    return result


def _default_anthropic_client():
    from anthropic import AsyncAnthropic
    api_key = settings.ANTHROPIC_API_KEY or None
    base_url = settings.ANTHROPIC_BASE_URL or None
    return AsyncAnthropic(api_key=api_key, base_url=base_url)


def _get_attr(obj, name: str, default=None):
    if isinstance(obj, dict):
        return obj.get(name, default)
    return getattr(obj, name, default)


def _normalize_content(content: Iterable[Any]):
    normalized = []
    for block in content:
        block_type = _get_attr(block, "type")
        if block_type == "text":
            normalized.append({"type": "text", "text": _get_attr(block, "text")})
        elif block_type == "tool_use":
            normalized.append({
                "type": "tool_use",
                "id": _get_attr(block, "id"),
                "name": _get_attr(block, "name"),
                "input": _get_attr(block, "input"),
            })
    return normalized
