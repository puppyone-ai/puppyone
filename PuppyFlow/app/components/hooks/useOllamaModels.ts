import { useState, useEffect, useCallback } from 'react';
import { Model } from '../states/AppSettingsContext';

interface OllamaModel {
  name: string;
  modified_at: string;
  size: number;
  digest: string;
  details?: {
    format: string;
    family: string;
    families: string[];
    parameter_size: string;
    quantization_level: string;
  };
}

interface OllamaResponse {
  models: OllamaModel[];
}

interface UseOllamaModelsOptions {
  endpoint?: string;
  autoFetch?: boolean;
  retryAttempts?: number;
  retryDelay?: number;
}

interface UseOllamaModelsReturn {
  models: Model[];
  rawModels: OllamaModel[];
  loading: boolean;
  error: string | null;
  isConnected: boolean;
  fetchModels: () => Promise<void>;
  refetch: () => Promise<void>;
  checkConnection: () => Promise<boolean>;
}

// æ ¼å¼åŒ–æ¨¡å‹åç§°ï¼Œè®©æ˜¾ç¤ºæ›´å‹å¥½
function formatModelName(modelId: string): string {
  // ç§»é™¤ç‰ˆæœ¬æ ‡ç­¾ï¼Œé¦–å­—æ¯å¤§å†™
  const baseName = modelId.split(':')[0];
  return baseName
    .split(/[-_]/)
    .map(word => word.charAt(0).toUpperCase() + word.slice(1))
    .join(' ');
}

// åˆ¤æ–­æ¨¡å‹ç±»å‹çš„å‡½æ•°
function detectModelType(modelName: string): 'llm' | 'embedding' {
  const lowerName = modelName.toLowerCase();

  // å¸¸è§çš„ embedding æ¨¡å‹å…³é”®è¯
  const embeddingKeywords = [
    'embed',
    'embedding',
    'bge',
    'e5',
    'sentence',
    'text-embedding',
    'nomic-embed',
    'mxbai-embed',
    'snowflake-arctic-embed',
  ];

  // æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦åŒ…å« embedding ç›¸å…³å…³é”®è¯
  const isEmbedding = embeddingKeywords.some(keyword =>
    lowerName.includes(keyword)
  );

  return isEmbedding ? 'embedding' : 'llm';
}

// å°† Ollama åŸå§‹æ¨¡å‹è½¬æ¢ä¸ºåº”ç”¨çš„ Model æ ¼å¼
function transformOllamaModel(ollamaModel: OllamaModel): Model {
  return {
    id: ollamaModel.name,
    name: formatModelName(ollamaModel.name),
    provider: 'ollama',
    isLocal: true,
    active: true,
    type: detectModelType(ollamaModel.name), // æ–°å¢ï¼šè‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
  };
}

export function useOllamaModels(
  options: UseOllamaModelsOptions = {}
): UseOllamaModelsReturn {
  const {
    endpoint = process.env.NEXT_PUBLIC_OLLAMA_ENDPOINT ||
      'http://localhost:11434',
    autoFetch = true,
    retryAttempts = 3,
    retryDelay = 1000,
  } = options;

  const [models, setModels] = useState<Model[]>([]);
  const [rawModels, setRawModels] = useState<OllamaModel[]>([]);
  const [loading, setLoading] = useState<boolean>(false);
  const [error, setError] = useState<string | null>(null);
  const [isConnected, setIsConnected] = useState<boolean>(false);

  // æ£€æŸ¥ Ollama æœåŠ¡è¿æ¥
  const checkConnection = useCallback(async (): Promise<boolean> => {
    try {
      const response = await fetch(`${endpoint}/api/tags`, {
        method: 'HEAD',
        signal: AbortSignal.timeout(3000),
      });
      const connected = response.ok;
      setIsConnected(connected);
      return connected;
    } catch (error) {
      setIsConnected(false);
      return false;
    }
  }, [endpoint]);

  // è·å–æ¨¡å‹åˆ—è¡¨ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
  const fetchModels = useCallback(async (): Promise<void> => {
    setLoading(true);
    setError(null);

    for (let attempt = 1; attempt <= retryAttempts; attempt++) {
      try {
        const response = await fetch(`${endpoint}/api/tags`, {
          method: 'GET',
          headers: {
            'Content-Type': 'application/json',
          },
          signal: AbortSignal.timeout(5000),
        });

        if (!response.ok) {
          throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }

        const data: OllamaResponse = await response.json();

        // æ‰“å° Ollama åŸç”Ÿ SDK è¿”å›çš„å®Œæ•´æ•°æ®
        console.log('ğŸ• Ollama Raw Response:', JSON.stringify(data, null, 2));

        const ollamaModels = data.models || [];
        const transformedModels = ollamaModels.map(transformOllamaModel);

        setRawModels(ollamaModels);
        setModels(transformedModels);
        setIsConnected(true);
        setError(null);
        return;
      } catch (err) {
        const errorMessage = err instanceof Error ? err.message : 'æœªçŸ¥é”™è¯¯';
        console.warn(
          `Ollama è¿æ¥å°è¯• ${attempt}/${retryAttempts} å¤±è´¥:`,
          errorMessage
        );

        if (attempt === retryAttempts) {
          setError(`ç»è¿‡ ${retryAttempts} æ¬¡å°è¯•åä»ç„¶å¤±è´¥: ${errorMessage}`);
          setIsConnected(false);
          setModels([]);
          setRawModels([]);
        } else {
          // ç­‰å¾…åé‡è¯•
          await new Promise(resolve => setTimeout(resolve, retryDelay));
        }
      }
    }
  }, [endpoint, retryAttempts, retryDelay]);

  // é‡æ–°è·å–ï¼ˆåˆ«åï¼Œä¸ºäº† API ä¸€è‡´æ€§ï¼‰
  const refetch = useCallback(() => fetchModels(), [fetchModels]);

  // è‡ªåŠ¨è·å–æ¨¡å‹åˆ—è¡¨
  useEffect(() => {
    if (autoFetch) {
      fetchModels();
    }
  }, [autoFetch, fetchModels]);

  // å®šæœŸæ£€æŸ¥è¿æ¥çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
  useEffect(() => {
    if (!autoFetch) return;

    const interval = setInterval(() => {
      if (!loading) {
        checkConnection();
      }
    }, 30000); // æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡

    return () => clearInterval(interval);
  }, [autoFetch, loading, checkConnection]);

  return {
    models,
    rawModels,
    loading,
    error,
    isConnected,
    fetchModels,
    refetch,
    checkConnection,
  };
}

// å¯¼å‡ºä¸€äº›æœ‰ç”¨çš„å·¥å…·å‡½æ•°
export { formatModelName, transformOllamaModel };
export type { OllamaModel, OllamaResponse, UseOllamaModelsOptions };
